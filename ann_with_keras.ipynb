{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks (ANNs): is a Machine Learning model inspired by the networks of biological neurons found in our brains.\n",
    "\n",
    "#### Biological Neuron\n",
    "![Biological Neuron](images/biological-neuron.jpg)\n",
    "\n",
    "\n",
    "Biological neurons produce short electrical impulses called *action potentials* (APs, or just *signals*) which travel along the axons and make the synapses release chemical signals called *neurotransmitters*. When a neuron receives suffient amount of these neurotransmitters within a few milliseconds, it fires its own electrical impulses.\n",
    "\n",
    "These individual neurons are organized in a vast network of billions, with each neuron typically connected to thousands of other neurons.\n",
    "\n",
    "### Logical computations with Neurons\n",
    "\n",
    "The below diagram represents the logical computations on neuron activation.\n",
    "\n",
    "![Logical Computations](images/log_comp.png)\n",
    "\n",
    "* If A is activated, then C also gets activated\n",
    "* C is activated only when both A and B are activated (logical *AND*)\n",
    "* C is activated if either A or B is activated (logical *OR*)\n",
    "* C is activated only if A is active and B is off.\n",
    "\n",
    "### The Perceptron\n",
    "The *Perceptron* is one of the simplest ANN architectures based on a slightly different artificial neuron called a *threshold logic unit* (TLU), or sometimes *lienar threshold unit* (LTU). the inputs and outputs are numbers (instead of binary on/off values, and each input connection is associated with a weight.\n",
    "\n",
    "The TLU computes the weighted sum of its inputs ($z = w_1x_1 + w_2x_2 +  ... + w_nx_n = x^Tw$), then applies a step function to that sum and outputs the result : $h_w(x)$ = step(z), where z = $x^Tw$.\n",
    "\n",
    "\n",
    "![TLU](images/tlu.png)\n",
    "\n",
    "\n",
    "The most common step function used in Perceptrons is the *Heaviside step function.* Sometimes the sign function is used instead.\n",
    "\n",
    " $$ heaviside(z)=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & if z < 0 \\\\\n",
    "      1 & if z >= 0 \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    " $$ sgn(z)=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      -1 & if z < 0 \\\\\n",
    "      1 & if z = 0 \\\\\n",
    "      +1 & if z > 0 \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "\n",
    "A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all inputs.  \n",
    "The inputs of the Perceptron are fed to special passthrough neurons called *input neurons*: they output whatever they are fed. All the input neurons form the *input layer*. Moreover, an extra bias feature is generally added (x$_0$ = 1): called a *bias neuron*, which outputs 1 all the time.\n",
    "\n",
    "![TLU](images/perceptron.png)\n",
    "\n",
    "\n",
    "The above Perceptron can classify instances simultaneously into three different binary classes, which makes it a multioutput classifier.\n",
    "\n",
    "Computing the outputs of a fully connected layer:\n",
    "\n",
    "$$h_{W,b}(X) = \\phi(XW + b)$$\n",
    "\n",
    "where,<br>\n",
    "X represents the matrix of input features,<br>\n",
    "W is weight matrix,<br>\n",
    "b is bias vector,<br>\n",
    "$\\phi$ is activation function (here, it is a *step function* as the artificial neurons are TLUs).\n",
    "\n",
    "### How is a Perceptron trained?\n",
    "\n",
    "The Perceptron training algorithm was largely inspired by *Hebb's Rule*.\n",
    "It states that the connection between two neurons tends to increse when they fire simultaneously. This later became known as Hebb's rule (or **Hebbian learning**). \n",
    "Perceptrons are trained taking into account the error made by the network when it makes a prediction; the Perceptron learning rule reinforces connnections that help reduce the error.\n",
    "\n",
    "Perceptron learning rule: $$ w_{i,j}^{(next step)} = w_{i,j} + \\eta (y_j - \\hat y_j) x_i $$\n",
    "\n",
    "In this equation:\n",
    "* $ w_{i,j}$ is the connection weight between the i$^{th}$ input neuron and the j$^{th}$ output neuron.\n",
    "* x$_i$ is the i$^{th}$ input value of the current training instance.\n",
    "* $y_j$ is the target output of the j$^{th}$ output neuron for the current training instance.\n",
    "* $\\hat y_j$ is the output of the  j$^{th}$ output neuron for the current training instance.\n",
    "* $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a single-TLU network with Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data[:, (2, 3)]\n",
    "y = (iris.target == 0).astype(np.int) # iris setosa\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are incapable of solving some trivial problems (e.g., the XOR classification problem). This limitation can be eliminated by stacking multiple Perceptrons. The resulting ANN is called a *Multilayer Perceptron* (MLP).\n",
    "\n",
    "### Multilayer Perceptron and Backpropagation\n",
    "\n",
    "An MLP is composed of one (passthrough) input layer, one or more layers of TLUs, called *hidden layers*, and one final layer of TLUs called the output layer. Every layer except for the output layer includes a bias neuron and is fully connected to the next layer.\n",
    "\n",
    "![MLP](images/mlp.png)\n",
    "\n",
    "When an ANN contains a deep stack of hidden layers, it is called a *deep neural network* (DNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
