{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks (ANNs): is a Machine Learning model inspired by the networks of biological neurons found in our brains.\n",
    "\n",
    "#### Biological Neuron\n",
    "![Biological Neuron](images/biological-neuron.jpg)\n",
    "\n",
    "\n",
    "Biological neurons produce short electrical impulses called *action potentials* (APs, or just *signals*) which travel along the axons and make the synapses release chemical signals called *neurotransmitters*. When a neuron receives suffient amount of these neurotransmitters within a few milliseconds, it fires its own electrical impulses.\n",
    "\n",
    "These individual neurons are organized in a vast network of billions, with each neuron typically connected to thousands of other neurons.\n",
    "\n",
    "### Logical computations with Neurons\n",
    "\n",
    "The below diagram represents the logical computations on neuron activation.\n",
    "\n",
    "![Logical Computations](images/log_comp.png)\n",
    "\n",
    "* If A is activated, then C also gets activated\n",
    "* C is activated only when both A and B are activated (logical *AND*)\n",
    "* C is activated if either A or B is activated (logical *OR*)\n",
    "* C is activated only if A is active and B is off.\n",
    "\n",
    "### The Perceptron\n",
    "The *Perceptron* is one of the simplest ANN architectures based on a slightly different artificial neuron called a *threshold logic unit* (TLU), or sometimes *lienar threshold unit* (LTU). the inputs and outputs are numbers (instead of binary on/off values, and each input connection is associated with a weight.\n",
    "\n",
    "The TLU computes the weighted sum of its inputs ($z = w_1x_1 + w_2x_2 +  ... + w_nx_n = x^Tw$), then applies a step function to that sum and outputs the result : $h_w(x)$ = step(z), where z = $x^Tw$.\n",
    "\n",
    "\n",
    "![TLU](images/tlu.png)\n",
    "\n",
    "\n",
    "The most common step function used in Perceptrons is the *Heaviside step function.* Sometimes the sign function is used instead.\n",
    "\n",
    " $$ heaviside(z)=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & if z < 0 \\\\\n",
    "      1 & if z >= 0 \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    " $$ sgn(z)=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      -1 & if z < 0 \\\\\n",
    "      1 & if z = 0 \\\\\n",
    "      +1 & if z > 0 \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "\n",
    "A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all inputs.  \n",
    "The inputs of the Perceptron are fed to special passthrough neurons called *input neurons*: they output whatever they are fed. All the input neurons form the *input layer*. Moreover, an extra bias feature is generally added (x$_0$ = 1): called a *bias neuron*, which outputs 1 all the time.\n",
    "\n",
    "![TLU](images/perceptron.png)\n",
    "\n",
    "\n",
    "The above Perceptron can classify instances simultaneously into three different binary classes, which makes it a multioutput classifier.\n",
    "\n",
    "Computing the outputs of a fully connected layer:\n",
    "\n",
    "$$h_{W,b}(X) = \\phi(XW + b)$$\n",
    "\n",
    "where,<br>\n",
    "X represents the matrix of input features,<br>\n",
    "W is weight matrix,<br>\n",
    "b is bias vector,<br>\n",
    "$\\phi$ is activation function (here, it is a *step function* as the artificial neurons are TLUs).\n",
    "\n",
    "### How is a Perceptron trained?\n",
    "\n",
    "The Perceptron training algorithm was largely inspired by *Hebb's Rule*.\n",
    "It states that the connection between two neurons tends to increse when they fire simultaneously. This later became known as Hebb's rule (or **Hebbian learning**). \n",
    "Perceptrons are trained taking into account the error made by the network when it makes a prediction; the Perceptron learning rule reinforces connnections that help reduce the error.\n",
    "\n",
    "Perceptron learning rule: $$ w_{i,j}^{(next step)} = w_{i,j} + \\eta (y_j - \\hat y_j) x_i $$\n",
    "\n",
    "In this equation:\n",
    "* $ w_{i,j}$ is the connection weight between the i$^{th}$ input neuron and the j$^{th}$ output neuron.\n",
    "* x$_i$ is the i$^{th}$ input value of the current training instance.\n",
    "* $y_j$ is the target output of the j$^{th}$ output neuron for the current training instance.\n",
    "* $\\hat y_j$ is the output of the  j$^{th}$ output neuron for the current training instance.\n",
    "* $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a single-TLU network with Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data[:, (2, 3)]\n",
    "y = (iris.target == 0).astype(np.int) # iris setosa\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are incapable of solving some trivial problems (e.g., the XOR classification problem). This limitation can be eliminated by stacking multiple Perceptrons. The resulting ANN is called a *Multilayer Perceptron* (MLP).\n",
    "\n",
    "### Multilayer Perceptron and Backpropagation\n",
    "\n",
    "An MLP is composed of one (passthrough) input layer, one or more layers of TLUs, called *hidden layers*, and one final layer of TLUs called the output layer. Every layer except for the output layer includes a bias neuron and is fully connected to the next layer.\n",
    "\n",
    "![MLP](images/mlp.png)\n",
    "\n",
    "When an ANN contains a deep stack of hidden layers, it is called a *deep neural network* (DNN).\n",
    "\n",
    "#### Backpropagation  training algorithm\n",
    "It can find out how each connection weight and each bias term should be tweaked in order to reduce the error.\n",
    "* It handles one mini-batch at a time, and it goes through full training set multiple times. Each pass is called an *epoch*.\n",
    "* Each mini-batch is passed to the network's input layer, which sends it to first hidden layer. The algorithm then computes the output of all neurons in this layer (for every instance in mini-batch). The result is passed on to next layer, and so on, until we get the output of the last layer. This is a *forward pass*.\n",
    "* Next, the algorithm measures the network's output error.\n",
    "* Then it computes how much each output connection contributed to the error (by applying *chain rule*).\n",
    "* The algorithm then measures how much of these error contributions come from each connection in the layer below, working backward until the algorithm reaches the input layer.\n",
    "* Finally, the algorithm performs Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
    "\n",
    "The step function is replaced with the logistic (sigmoid) function, $\\sigma(z) = 1 / (1 + exp(-z)) $. This was essential because the step function contains only flat segments, so there is no gradient to work with (Gradient Descent cannot move on a flat surface).\n",
    "\n",
    "Backpropagation also works well with other activation functions like:<br>\n",
    "* The hyperbolic tangent function : tanh(z) = 2 $\\sigma$(2z) - 1\n",
    "* The Rectified Linear Unit function: ReLU(z) = max(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Image Classifier using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), dtype('uint8'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape, X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a validation set and scaling the pixel intensities to the 0-1 range by dividing with 255, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are the class IDs (represented as uint8), from 0 to 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 7, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the corresponding class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first image in the training set is a coat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure fashion_mnist_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x345.6 with 40 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "save_fig('fashion_mnist_plot', tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build the neural network! Here is a classification with two hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "model.add(keras.layers.Dense(300, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Flatten layer converts each input image into a 1D array (it computes X.reshape(-1, 1)). This layer is just there to do simple preprocessing.\n",
    "* A Dense output layer with 10 neurons(one per class), uses softmax activation function which ensures that all the estimated probabilities are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# displays all the model's layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In output shape: None means the batch size can be anything.<br>\n",
    "The first hidden layer has 784 x 300 connection weights, plus 300 bias terms, which adds up to 235500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x7f9ce9d6b640>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f9cebc23d90>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f9cebc23c70>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f9cebd906a0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f9cebc23c70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.01003801, -0.04325436, -0.04484243, ...,  0.01199073,\n",
       "          0.01148185,  0.05522171],\n",
       "        [ 0.01244114,  0.06767683,  0.06142695, ...,  0.01430641,\n",
       "          0.05937132, -0.02304103],\n",
       "        [-0.06236849,  0.06793554,  0.02341427, ...,  0.02577329,\n",
       "         -0.05445239, -0.02746987],\n",
       "        ...,\n",
       "        [-0.06234135,  0.0195537 ,  0.01910534, ...,  0.05252483,\n",
       "         -0.06795444,  0.05522694],\n",
       "        [-0.07417506, -0.03450098,  0.03074972, ..., -0.00975307,\n",
       "          0.0610166 ,  0.04502045],\n",
       "        [ 0.00284652, -0.02774149,  0.02018623, ...,  0.01522365,\n",
       "          0.02362687,  0.01185147]], dtype=float32),\n",
       " (784, 300))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " (300,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases, biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7107 - accuracy: 0.7648 - val_loss: 0.5466 - val_accuracy: 0.8052\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4909 - accuracy: 0.8282 - val_loss: 0.4374 - val_accuracy: 0.8528\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4440 - accuracy: 0.8445 - val_loss: 0.4233 - val_accuracy: 0.8560\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4155 - accuracy: 0.8539 - val_loss: 0.3963 - val_accuracy: 0.8670\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3969 - accuracy: 0.8599 - val_loss: 0.3911 - val_accuracy: 0.8716\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3806 - accuracy: 0.8664 - val_loss: 0.3849 - val_accuracy: 0.8630\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3665 - accuracy: 0.8708 - val_loss: 0.3624 - val_accuracy: 0.8776\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3546 - accuracy: 0.8747 - val_loss: 0.3638 - val_accuracy: 0.8702\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3450 - accuracy: 0.8772 - val_loss: 0.3487 - val_accuracy: 0.8728\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.3346 - accuracy: 0.8799 - val_loss: 0.3405 - val_accuracy: 0.8772\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3266 - accuracy: 0.8841 - val_loss: 0.3525 - val_accuracy: 0.8740\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.3185 - accuracy: 0.8865 - val_loss: 0.3430 - val_accuracy: 0.8746\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3113 - accuracy: 0.8890 - val_loss: 0.3400 - val_accuracy: 0.8794\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3045 - accuracy: 0.8903 - val_loss: 0.3371 - val_accuracy: 0.8756\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2982 - accuracy: 0.8919 - val_loss: 0.3177 - val_accuracy: 0.8838\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2917 - accuracy: 0.8950 - val_loss: 0.3684 - val_accuracy: 0.8612\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2859 - accuracy: 0.8961 - val_loss: 0.3135 - val_accuracy: 0.8882\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2811 - accuracy: 0.8989 - val_loss: 0.3122 - val_accuracy: 0.8888\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2761 - accuracy: 0.8999 - val_loss: 0.3207 - val_accuracy: 0.8802\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2704 - accuracy: 0.9024 - val_loss: 0.3069 - val_accuracy: 0.8910\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2653 - accuracy: 0.9044 - val_loss: 0.3017 - val_accuracy: 0.8900\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2611 - accuracy: 0.9057 - val_loss: 0.3074 - val_accuracy: 0.8862\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2569 - accuracy: 0.9065 - val_loss: 0.3047 - val_accuracy: 0.8878\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2515 - accuracy: 0.9087 - val_loss: 0.3010 - val_accuracy: 0.8912\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2472 - accuracy: 0.9109 - val_loss: 0.2916 - val_accuracy: 0.8960\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2440 - accuracy: 0.9122 - val_loss: 0.2918 - val_accuracy: 0.8946\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2391 - accuracy: 0.9129 - val_loss: 0.2996 - val_accuracy: 0.8906\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2347 - accuracy: 0.9148 - val_loss: 0.2975 - val_accuracy: 0.8912\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2309 - accuracy: 0.9164 - val_loss: 0.3041 - val_accuracy: 0.8916\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2275 - accuracy: 0.9182 - val_loss: 0.3083 - val_accuracy: 0.8864\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 1, 'epochs': 30, 'steps': 1719}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.7107414603233337,\n",
       "  0.49090835452079773,\n",
       "  0.44401320815086365,\n",
       "  0.4154742658138275,\n",
       "  0.39689818024635315,\n",
       "  0.3805648386478424,\n",
       "  0.366514652967453,\n",
       "  0.3546100854873657,\n",
       "  0.34498628973960876,\n",
       "  0.33463746309280396,\n",
       "  0.3265800178050995,\n",
       "  0.3185178339481354,\n",
       "  0.3112618625164032,\n",
       "  0.30451586842536926,\n",
       "  0.2982017695903778,\n",
       "  0.2916739583015442,\n",
       "  0.28591254353523254,\n",
       "  0.2810872495174408,\n",
       "  0.27614620327949524,\n",
       "  0.2703799307346344,\n",
       "  0.26532232761383057,\n",
       "  0.2611219882965088,\n",
       "  0.25688424706459045,\n",
       "  0.2514587938785553,\n",
       "  0.24717961251735687,\n",
       "  0.24403443932533264,\n",
       "  0.23909910023212433,\n",
       "  0.23470968008041382,\n",
       "  0.23092561960220337,\n",
       "  0.22752828896045685],\n",
       " 'accuracy': [0.7648000121116638,\n",
       "  0.8281999826431274,\n",
       "  0.8444545269012451,\n",
       "  0.8538545370101929,\n",
       "  0.8599091172218323,\n",
       "  0.8663636445999146,\n",
       "  0.8707818388938904,\n",
       "  0.8747272491455078,\n",
       "  0.8772181868553162,\n",
       "  0.8798909187316895,\n",
       "  0.8841090798377991,\n",
       "  0.8865454792976379,\n",
       "  0.8890363574028015,\n",
       "  0.8903272747993469,\n",
       "  0.891945481300354,\n",
       "  0.8950181603431702,\n",
       "  0.8961091041564941,\n",
       "  0.8989454507827759,\n",
       "  0.8999090790748596,\n",
       "  0.902363657951355,\n",
       "  0.9044181704521179,\n",
       "  0.9057272672653198,\n",
       "  0.9064909219741821,\n",
       "  0.9087272882461548,\n",
       "  0.9108909368515015,\n",
       "  0.9121999740600586,\n",
       "  0.9129454493522644,\n",
       "  0.9147818088531494,\n",
       "  0.9163818359375,\n",
       "  0.918218195438385],\n",
       " 'val_loss': [0.5466094017028809,\n",
       "  0.43736180663108826,\n",
       "  0.42331627011299133,\n",
       "  0.3962877094745636,\n",
       "  0.39111509919166565,\n",
       "  0.3849168121814728,\n",
       "  0.3623729348182678,\n",
       "  0.3638359010219574,\n",
       "  0.348720908164978,\n",
       "  0.3404642641544342,\n",
       "  0.35250577330589294,\n",
       "  0.34298914670944214,\n",
       "  0.34003934264183044,\n",
       "  0.3370782434940338,\n",
       "  0.31773215532302856,\n",
       "  0.36838313937187195,\n",
       "  0.31353524327278137,\n",
       "  0.3121955096721649,\n",
       "  0.3207499384880066,\n",
       "  0.30685171484947205,\n",
       "  0.30172961950302124,\n",
       "  0.3074299693107605,\n",
       "  0.3046833872795105,\n",
       "  0.30100351572036743,\n",
       "  0.2915707230567932,\n",
       "  0.2917606830596924,\n",
       "  0.29955199360847473,\n",
       "  0.29753378033638,\n",
       "  0.3041045367717743,\n",
       "  0.3083016574382782],\n",
       " 'val_accuracy': [0.8051999807357788,\n",
       "  0.8528000116348267,\n",
       "  0.8560000061988831,\n",
       "  0.8669999837875366,\n",
       "  0.8715999722480774,\n",
       "  0.8629999756813049,\n",
       "  0.8776000142097473,\n",
       "  0.870199978351593,\n",
       "  0.8727999925613403,\n",
       "  0.8772000074386597,\n",
       "  0.8740000128746033,\n",
       "  0.8745999932289124,\n",
       "  0.8794000148773193,\n",
       "  0.8755999803543091,\n",
       "  0.8838000297546387,\n",
       "  0.8611999750137329,\n",
       "  0.8881999850273132,\n",
       "  0.8888000249862671,\n",
       "  0.8802000284194946,\n",
       "  0.890999972820282,\n",
       "  0.8899999856948853,\n",
       "  0.8862000107765198,\n",
       "  0.8877999782562256,\n",
       "  0.8912000060081482,\n",
       "  0.8960000276565552,\n",
       "  0.894599974155426,\n",
       "  0.8906000256538391,\n",
       "  0.8912000060081482,\n",
       "  0.8916000127792358,\n",
       "  0.8863999843597412]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure keras_learning_curves_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "save_fig(\"keras_learning_curves_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 57.9604 - accuracy: 0.8462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[57.960357666015625, 0.8461999893188477]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-32-9634b1d2cb54>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure fashion_mnist_images_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAACUCAYAAADVqv1WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXNklEQVR4nO3de5AVVX4H8O9PBXkPwiDIyA6FgLqioFXB4BPFKgVRV/ehlouazRJXK7ESY4qKUVaTGCq6VVF3Y4wbX6msWD6w1CRExPUFCLpBEUVeDgwIyvsxgiDqyR+3Z3PPtw+3ey4znJ7h+6maYn730d1Mnztn+vz6d4455yAiInKwHRb7AERE5NCkDkhERKJQByQiIlGoAxIRkSjUAYmISBTqgEREJIoO1QGZmTOzoS19LmOb15vZnAM/OmlP+LxX235EZP8K2QGZ2etmts3Mjox9LG3FzMaa2aexj+NQYGarzexLM/vCzDaY2WNm1iP2cUnxJW2m+evbsnb0hZldE/v42rvCdUBmNhjA2QAcgEujHox0JJc453oAOA3AHwC4PfLxVGRmR8Q+BgGccz2avwCsQdKOkq/fNL+uCOerCMfQUoXrgABcC2A+gMcBXFf+hJk9bmb/bGb/ZWZNZrbAzI4LbcTMzjKztWZ2XuC5I83sF2a2JvmL+CEz61rhmMzMfmlmO8xsqZmNK3tioJm9aGZbzWylmU2m/dxnZuuTr/uSx7oDmAlgYNlfUwNb9FOSqjjn1qH0sx+RDKv9/kObXHn/NGsbZlZjZv9uZpvMrNHMbjezw5Jzu93MRpS9tl/yV/PRSTzRzN5PXjfPzE4pe+1qM5tiZh8A2NUef6EcKppHMJLz9TmAx/b3eU9enxrKLx/WNbMJZrYk+b22zsxuLXtdh20zRe2AfpN8XWhm/en5qwHcBeAoACsB3M0bMLMLAUwH8H3n3GuBffwjgOEARgEYCqAOwNQKx3Q6gAYAtQB+DmCGmfVJnpsO4FMAAwH8AMA/lHVQfwPgD5P9jAQwGsDtzrldAMYDWF/219T6CvuXVmJmgwBMALDtADbzSwA1AIYAOBelNvtHzrm9AGag1Eab/QjAG865jWZ2GoBHAdwAoC+AfwXwIg01Xw3gYgC9nXNfH8AxStsbAKAPgHoAf4L9fN5zbusRADc453oCGAHgtwDQ4duMc64wXwDOArAPQG0SLwXwF2XPPw7g38riCQCWlsUOwF8DaARwMm3bodTZGIBdAI4re24MgFX7OabrAawHYGWPvQNgEoBBAL4B0LPsuWkAHk++/wTAhLLnLgSwOvl+LIBPY//MD4UvAKsBfAFge9I2HgRwYtImjih73esAflp23ucE2s/hAPYC+G7ZczcAeD35/gIADWXPzQVwbfL9vwD4Ozq2ZQDOLTvOn8T+eemrYju6IPl+LICvAHQpe77S591rT+VtKvl+TdKOetFrOnSbKdoV0HUAZjnnNifxk6BhOACfl32/GwAnk/8cwNPOucX72Uc/AN0A/G9ySbsdwP8kj+/POpec7UQjSlc8AwFsdc410XN1yfcDk5jfJwff95xzvZ1z9c65mwB8WeV2agF0Rvq8Np/z3wLoamanm1k9Sn8NP588Vw/gL5vbXdL2BsFvE2urPC45+DY55/aUxQfyef8+Sn9QN5rZG2Y2Jnm8Q7eZwowXJjmYHwE4PBlTBYAjAfQ2s5HOuUU5N/VDAI+Y2Trn3H2B5zej9MvnJFfKB+RRZ2ZW1gl9B8CLKF0Z9TGznmWd0HcANG93PUoN6KOy55qH2jQNeVy7kn+7AdiZfD8gx/s2o3SVXg9gSfLY78+5c+5bM3sapWGRDQD+s6xtrAVwt3MuNWxcRu2i/eBzVenzvgultgYAMDOvrTnn3gVwmZl1AvCnAJ5GqaPp0G2mSFdA30NpOOu7KP3VOAqlYZK3UBpjz2s9gHEAbjazm/hJ59y3AH4N4J/KEsN1Sd5of45OttfJzH6YHNd/O+fWApgHYJqZdUmSg3+MUv4KKOWHbk8S0bUo5Zn+I3luA4C+ZlbTgv+btBLn3CaUOo0fm9nhZvYTAMEbWuh936D0y+FuM+uZXOXcgv8/r0Dpyv1KANck3zf7NYCfJVdHZmbdzexiM+vZSv8tiavS530RgJPMbJSZdQFwZ/ObzKyzmV1jZjXOuX0o/UH0TfJ0h24zReqArgPwmHNujXPu8+YvAL8CcE1L7u5wzq1BqROasp+7mqagdAPDfDPbCWA2gOMrbHIBgGEo/fV7N4AfOOe2JM9dDWAwSh3f8wB+7px7JXnu7wH8DsAHABYDWJg8BufcUpQabENyaa2huYNvMoC/ArAFwEko/TGRx5+h9BdtA4A5KHUyjzY/6ZxbkDw/EKU77pof/12yz1+hdBPESpRyA9IxVPq8Lwfwtyj9rlmBUrspNwnA6uT30c8A/Dh5X4duM+anNkRERA6OIl0BiYjIIUQdkIiIRKEOSEREolAHJCIiUagDEhGRKLJubdYtch2XteG220W7aWpqSj32zjvvePG4ceNSr2mphQsXenGPHv7kHcOHDz/gfRxEHb7d8J3BZv5/+dVXX02954EHHvDiUaNGefHnn3/uxUOHppeW+uKLL7x42zZ/usIjjvB/Xa9atSq1jeeffz71WEEE242ugEREJAp1QCIiEkVWIWohLomlTXS4oZQ9e/Z48X33+VMBTp8+3Yt5iAMANm3a5MVdu/rLRIXek6VLly4VYx5aAYBzzjnHiydPnuzFF110UYuPo5V0uHbDvv32Wy8+7DD/7/Szzjor9Z65c+e2aB+9evVKPbZ7924v/vprf2UFbotffpmeT/ell17y4okTJ7bouNqQhuBERKQ41AGJiEgU6oBERCQK5YAOXe16LH/KlCmpxx5++GEv3rlzpxd369bNi3lMHUjnY3icfd++fV78zTffgB155JFezPvhz9zevXtT2+D98n7GjBnjxW+++WZqG22kXbeb1tCzZ3olhE6dOnlxv37++pa7du3y4lC74dwgb5PbzcqVK1PbuPfee7341ltvTb0mEuWARESkONQBiYhIFOqAREQkCnVAIiISRe5lrkVi4hsM7rnnntRrBgwY4MXdu3f3Yp7TK3QDDt9kkFVEytsE0oWLXFDIeJtAer64ww8/3Iu58PGSSy5JbYOLEqV18JxtAFBbW+vFfAMMF7fyjSqh1/B+Qu9ha9euzXxNkegKSEREolAHJCIiUagDEhGRKJQDknbhjjvu8OLQZI6cj+FiP16TJaR3795enDVxaCgfwJOi9u3bt+JxhSYj5eJUzlf179/fi0OFqJs3b/ZizlNIPhs2bMh8DZ/DUG6wXCgvyIWnnPfjbYY+Axs3bqy436LRFZCIiEShDkhERKJQByQiIlEoByTtwo4dO7w4VBPBeRLO+dx4441efMMNN6S2cdppp3kx1xJ9+umnXhyamLK+vt6LOYfAx87bBIC6urqK72lqavLi0OJkDQ0NXqwcUHU+/PDDzNd07tzZi/l8cD4nlPfjOiBuz3lqiTjvV3S6AhIRkSjUAYmISBTqgEREJArlgKRd4LqY0PxpGYsrYtq0aV5cU1OTeg2Ps+/evduLx44d68WvvfZaxX0CwIknnujFS5cu9WKeNwwA7r//fi/mOihe8Cy0wNmcOXO8ePTo0ZnHKmmLFi3yYs73AOn2yO2Ga8M4pwmk68Wy5i4MLWTIOcui0xWQiIhEoQ5IRESiUAckIiJRqAMSEZEodBNCG+PkMC9WljVpIZBONnIB2ooVK7x42LBhLTnEQvrqq68qPh/6uYWSsuWuvfZaL37hhRcyj2Pbtm1ezDcdTJ06NfUeniTyqaee8uKtW7d6cWNjY2obV155pRfzTQh5JjR9//33U49Jy7377rtezJ9hIH3TAZ8PvumAC56B9Pk66qijvJg/97xPABg0aFDqsSLTFZCIiEShDkhERKJQByQiIlEcsjkgLuoKFTHyWO+6deu8+O233/bi8ePHp7bRGoVhoUkHy82YMcOLp0yZcsD7jG39+vUVnw+Nw4cm5CwXmvQzyzPPPFPx+UmTJqUe69q1qxdzvmbkyJFe/Nlnn6W20aNHj7yHuF+cG5TqfPzxx17MC8cB6fbICxUec8wxXjx//vzUNjivyUXRHIcWtevTp0/qsSLTFZCIiEShDkhERKJQByQiIlEcsjkgFsopsLfeesuLFyxY4MWhvMXNN998YAcGYOPGjV788ssve3FoUbT2btOmTS1+D4+J81g9nx8eUw8599xzKz5/4YUXph5btWqVF/O4/MyZM72YJzgF0nkizgnxsfOCZ0B6QT6pDtfwhH7WWTmgK664osX75fbcrVu3zPdk1c8Vja6AREQkCnVAIiIShTogERGJ4pDNAeWZS4vngOJ6gP79+3txqO7i8ssv92Ke34kXqqqvr09tY8uWLV7MC5jV1dWl3tPecc0Vy1p8DkiPmXNOJJT34+0uW7bMi7nGqqGhIfM4shakW7NmTeo9Dz74oBdz3UjWPGFA9s9Q8tmwYYMXV1Pbd/XVV2e+hs8hzxlYW1ubuY3Q/HBFpisgERGJQh2QiIhEoQ5IRESiUAckIiJRHDI3IXDhHt90sGvXrtR7nn32WS/mJCHfQNDU1JTaRtakpxx/9NFHqW0ce+yxXswJaL6hoiPIKkQNFQNy4R7HXMx52223ZW5j1qxZXrxo0SIvDp0vvkmEbzrgGxl48TkgezE5bs+hBfr27dtXcRuSD09yGyr8zvoMnnfeeZn7GTNmjBfzZMehyUdZ3759M19TJLoCEhGRKNQBiYhIFOqAREQkiug5oFBBYdbCTPx8aPybx2RDOYNyDz30UOoxLjTt0qWLFzc2Nnox54RC2+BxXD72UJEb5554csS9e/d6cSif1RoL4x1MoUXayuUpIuWfdU1NjRdPmzYt8zj4PXw+lyxZkrmNAQMGePHmzZu9mNtVHnkKqbPek/WZkPw438bnI2tRSQAYPHiwF8+ZM8eL8xRfc3stOl0BiYhIFOqAREQkCnVAIiISRZvngHjcMk/+hmUtFhe6Bz9rfHv69OleHFq869RTT/Vizils377di3nhMSB9Xz6P//PCVXnu9eefKU9AGJoUddSoUZnbLZJqFqTr3LmzF59//vlezAsKcn0VkG43nF/jtsa1RSF8TjmPxPsIbbd3795ezHVCobbHVq9e7cXHHXdc5nskLfQ7ixeCq+Zny+2R21qe35Xtja6AREQkCnVAIiIShTogERGJos1zQFnjllzjE3qMx+V5m3nqGR599FEvXr58uRcPGjQo9R5eCI5zLzxHVGhhOJ4fjo+dF00L1RJl5dHYyy+/nHqsveWAOL/GQvPu8c//+uuv9+KZM2d6Mf/sQ7gthtprFj5fnBMK5YC4juSKK67w4qy54kI4/6gcUHVCNVdce3fSSSe1eLsTJkzw4nvuuceLq2l7RacrIBERiUIdkIiIRKEOSEREolAHJCIiURzQTQh5kmKcgOWEeqjINKvwlK1fvz712IwZM7yYbxgYNmyYF3NBKJBODvNNCZ06dfLi0M0BXCTK+P8amrSQX8MTi/J+586dW3Gf7QH/rBmfTwA4+uijvZgX7mN8/oDsyWJb2jZD28hTYMht7/TTT6+4j9Bx8SSnHTGJHUOo8J1/rw0ZMqTF2x05cqQXc3FrniL19jbpsK6AREQkCnVAIiIShTogERGJomIOKGsBq9YYDw/hiSh5EsVly5Z5cWjxMp6YslevXl7MhY47d+5MbYMXmeJxef558HEC6XFbnlSSjzPP+HLXrl0rvic0QeaHH37oxSNGjEi9pkj4/HA+I1Swy+PfH3/8ccV9hAoK+ZyzaiaErGZCXv7/V1PQzfvlQlTJhycJDS34yL8LBw4c2OL9ZC0qqByQiIhIK1EHJCIiUagDEhGRKCoOOmZN8rlhw4bUY42NjV7M46Uch+o5Vq1a5cVcS8NjpT179kxtg8fEd+zYUXG/ofFX3i/nXrhmh+/bB4BjjjnGiznXxPsI1a5wjdLWrVu9mHM+ocX1+D1FV03NyvHHH+/Fn3zyScXXh/IqvN+sOrY8siYjDdV+8X64xonlyQFVs8ifpH/2DQ0NqdfwOeXJjvPgfDDLyhEB2XWHRaMrIBERiUIdkIiIRKEOSEREomjRXHCzZ8/24tAcbDxOyePOWbVFoW1wjodzIqGcB49/cw0P51pCY+i8Hz52vuc+VH/DdT/VjMPzsXLNAeezQrmoPOPHRcL1OHmOn3NAb7zxRsXX56mr4HbE7SRPLRxvg+M8CypyLQrHeWp8QvMdSrbRo0d7cai+jPN41SwYmCW0cGHWcRSdroBERCQKdUAiIhKFOiAREYlCHZCIiERRMbM7a9YsL37kkUe8+IQTTki9hwsv+QYCTuKGiq842c9JW95mKOnOyeGmpqaK2wwVxGYtJMY3P4QKc5csWVLxWEOTjzK+uYGLeXmiztDNEFmFjEXDRb95EvV8zpcuXerFvABdnp99NbIWnOM4zw0WK1eu9OIBAwZ4cehGHP7/trcixaI455xzvPixxx5LvYZ/j7333nsHvF9uz3lumqlmguiY2tfRiohIh6EOSEREolAHJCIiUVQcfOYCrPnz53vx4sWLU++ZM2dOxR3yuHRoItE+ffpUjGtqarw4lAPiHM+WLVu8mBe1C42P88ShPHa/aNEiLz7llFNS2xg8eLAXv/LKK17MxWV5xnA5Z8CLX/Hie0A6B1Z0/H/Mk6/h4lWegLVbt25eXM2Ep6yaBeo4n5VnbP+FF17wYm5XCxcuTL2H29K2bdtyHqGUO+OMM7yYc65A+py2Rs6VP8d5JsJtjTZ9MOkKSEREolAHJCIiUagDEhGRKCrmgHgizalTp2ZukCc8XLBggRdz7mXevHmpbaxevdqLP/jgAy/mOpjQ2CiPzfN4OOeVTj755NQ2LrjgAi+eMGGCF4fGgrNceumlXrxmzRov7tu3b+o9PBbMeTPOl4QmJBw+fHiLjjM2Pl979uzJfA/X/XB+jX8unDMC0mP5WePuoef5saw8UZ5xe/5McL7x2WefTb2H9xv6/0q2+vp6Lw7lWLmtcXvlReyGDBmSuV/Ol+c5f21V29ZWdAUkIiJRqAMSEZEo1AGJiEgUrb5KGc9DNm7cuIrxTTfd1NqHUGgvvvhi7ENoFzhfkydPwnUuPA7P26xmfjmOQ/mdrLnfshaoA9K1bm+//bYX58np8X5D8x1Ky4UWhuNaLq5NrCYHxPNqch6QF6oElAMSERHJRR2QiIhEoQ5IRESiUAckIiJRtPpNCCKtgYvweCJRLngGgFtuucWLZ8+e7cWchK9m8a6sGwyA7OJVvqEidBw7duzw4rFjx3rxxIkTvfiuu+5KbYNvsgglzyUtq5D48ssvT73nySef9GI+xzxJMxe5h3CbzzpOIHxjQpHpCkhERKJQByQiIlGoAxIRkSiUA5JC4glnOZ/BOSIgPVljv379vHjFihVeHCoGbIsFvbJyCqH/CxfV8gJntbW1mfvl3FJjY2PmeyT7fF122WWp9zzxxBNe3LlzZy9+7rnnvPjOO+/MPA4uKs2TfwxNRFxkugISEZEo1AGJiEgU6oBERCQK5YCkkM4880wv5sk4Q4sB8gSdy5cvb/0DKwie3JIXKQTSdT+jR49u02PqKLLqtMaPH596D9ff8M++mpqzESNGePHixYu9OPQZ+Oyzz1q8n5h0BSQiIlGoAxIRkSjUAYmISBTKAUkhcb6C53HjOgugunH29oprnkLzvPGiaN27d2/TY+oo8ixUyOrr6714/vz5Xrx7924vnjdvXmobZ5xxhhdzHRAvsMjnFwA2b96cfbAFcuh8YkVEpFDUAYmISBTqgEREJAp1QCIiEoVuQpBCqqur8+JTTz3Vi0NFeFlJ9q+//tqLQ8nmrMXkDhY+Dj7WoUOHevHFF1+c2sb27du9eMyYMa10dB1baJLPLJMnT/biE044wYuvuuoqL+YbDkImTZrkxbxIYY8ePVLvOfvsszO3WyS6AhIRkSjUAYmISBTqgEREJAorypi3iIgcWnQFJCIiUagDEhGRKNQBiYhIFOqAREQkCnVAIiIShTogERGJ4v8AkM6yWdjYHs4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 518.4x172.8 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7.2, 2.4))\n",
    "\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "save_fig('fashion_mnist_images_plot', tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Regression MLP using the Sequential API\n",
    "Let's load, split and scale the California housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is similar to classification example. The main differences are the fact that the output layer has a single neuron and uses no activation function, and the loss function is the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]), #  input_shape = (11610, 8)[1:]\n",
    "    keras.layers.Dense(1) # single neuron output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.4230 - val_loss: 0.5546\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4911 - val_loss: 0.4918\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4526 - val_loss: 0.4741\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4374 - val_loss: 0.4587\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4251 - val_loss: 0.4405\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4167 - val_loss: 0.4423\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.4369\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4057 - val_loss: 0.4257\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3995 - val_loss: 0.4265\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3952 - val_loss: 0.4208\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3895 - val_loss: 0.4297\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.4218\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.4040\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.4038\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.4067\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3718 - val_loss: 0.4002\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3682 - val_loss: 0.3964\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.3890\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.3978\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3601 - val_loss: 0.3842\n",
      "162/162 [==============================] - 0s 782us/step - loss: 0.3689\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59607327],\n",
       "       [1.6410043 ],\n",
       "       [3.7451282 ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3] # sample data as new instances for prediction\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Complex models using the Functional API\n",
    "\n",
    "#### Handling multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_) \n",
    "hidden2 = keras.layers.Dense(10, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.5138 - val_loss: 0.7666\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6643 - val_loss: 0.6318\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6033 - val_loss: 0.6059\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5816 - val_loss: 0.5874\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5641 - val_loss: 0.5719\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5506 - val_loss: 0.5594\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5398 - val_loss: 0.5501\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5308 - val_loss: 0.5428\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 0.5361\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5177 - val_loss: 0.5306\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5125 - val_loss: 0.5258\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5082 - val_loss: 0.5224\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5047 - val_loss: 0.5197\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5014 - val_loss: 0.5154\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4983 - val_loss: 0.5144\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4954 - val_loss: 0.5115\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4942 - val_loss: 0.5080\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4911 - val_loss: 0.5069\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4893 - val_loss: 0.5045\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4872 - val_loss: 0.5034\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 793us/step - loss: 0.5063\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7520311],\n",
       "       [1.596787 ],\n",
       "       [2.9141204]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_new)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, we want to send five features through the wide path (0 to 4), and six features through the deep path (2 to 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='output')(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.5665 - val_loss: 1.1959\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.9517 - val_loss: 0.7852\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7212 - val_loss: 0.6726\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6381 - val_loss: 0.6209\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5934 - val_loss: 0.5877\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5641 - val_loss: 0.5672\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5438 - val_loss: 0.5501\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5290 - val_loss: 0.5383\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.5311\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5105 - val_loss: 0.5244\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5042 - val_loss: 0.5194\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4992 - val_loss: 0.5155\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4949 - val_loss: 0.5114\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4915 - val_loss: 0.5084\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4882 - val_loss: 0.5068\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4851 - val_loss: 0.5044\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4829 - val_loss: 0.5012\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4802 - val_loss: 0.4984\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4780 - val_loss: 0.4971\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4757 - val_loss: 0.4955\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_val_A, X_val_B = X_val[:, :5], X_val[:,2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_val_A, X_val_B), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 833us/step - loss: 0.4872\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55146885],\n",
       "       [1.80619   ],\n",
       "       [2.9147563 ]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict((X_new_A, X_new_B))\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output will need its own loss function (if we pass single loss, Keras will assume the same loss for all the outputs).<br>\n",
    "We care much more about the main output than about the auxilary output (as it is just used for regularization), so we want to give the main output's loss a much greater weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.9276 - output_loss: 0.8249 - aux_output_loss: 1.8514 - val_loss: 0.6587 - val_output_loss: 0.5801 - val_aux_output_loss: 1.3660\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6985 - output_loss: 0.6355 - aux_output_loss: 1.2651 - val_loss: 0.5974 - val_output_loss: 0.5340 - val_aux_output_loss: 1.1677\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5713 - output_loss: 0.5167 - aux_output_loss: 1.0621 - val_loss: 0.5454 - val_output_loss: 0.4944 - val_aux_output_loss: 1.0046\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5097 - output_loss: 0.4641 - aux_output_loss: 0.9202 - val_loss: 0.5145 - val_output_loss: 0.4749 - val_aux_output_loss: 0.8713\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4862 - output_loss: 0.4509 - aux_output_loss: 0.8044 - val_loss: 0.4946 - val_output_loss: 0.4629 - val_aux_output_loss: 0.7800\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4725 - output_loss: 0.4428 - aux_output_loss: 0.7399 - val_loss: 0.4874 - val_output_loss: 0.4602 - val_aux_output_loss: 0.7316\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4573 - output_loss: 0.4312 - aux_output_loss: 0.6926 - val_loss: 0.4723 - val_output_loss: 0.4469 - val_aux_output_loss: 0.7012\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4499 - output_loss: 0.4261 - aux_output_loss: 0.6640 - val_loss: 0.5244 - val_output_loss: 0.5018 - val_aux_output_loss: 0.7279\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4378 - output_loss: 0.4154 - aux_output_loss: 0.6392 - val_loss: 0.4562 - val_output_loss: 0.4340 - val_aux_output_loss: 0.6555\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4303 - output_loss: 0.4093 - aux_output_loss: 0.6184 - val_loss: 0.4686 - val_output_loss: 0.4497 - val_aux_output_loss: 0.6388\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4299 - output_loss: 0.4103 - aux_output_loss: 0.6055 - val_loss: 0.4507 - val_output_loss: 0.4317 - val_aux_output_loss: 0.6211\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4260 - output_loss: 0.4078 - aux_output_loss: 0.5902 - val_loss: 0.4502 - val_output_loss: 0.4304 - val_aux_output_loss: 0.6283\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4140 - output_loss: 0.3953 - aux_output_loss: 0.5818 - val_loss: 0.4988 - val_output_loss: 0.4860 - val_aux_output_loss: 0.6134\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4033 - output_loss: 0.3850 - aux_output_loss: 0.5681 - val_loss: 0.4187 - val_output_loss: 0.4001 - val_aux_output_loss: 0.5863\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3953 - output_loss: 0.3774 - aux_output_loss: 0.5558 - val_loss: 0.4187 - val_output_loss: 0.3991 - val_aux_output_loss: 0.5952\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3892 - output_loss: 0.3718 - aux_output_loss: 0.5459 - val_loss: 0.4292 - val_output_loss: 0.4110 - val_aux_output_loss: 0.5925\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3837 - output_loss: 0.3669 - aux_output_loss: 0.5341 - val_loss: 0.4020 - val_output_loss: 0.3843 - val_aux_output_loss: 0.5618\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3789 - output_loss: 0.3626 - aux_output_loss: 0.5260 - val_loss: 0.4112 - val_output_loss: 0.3945 - val_aux_output_loss: 0.5615\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3743 - output_loss: 0.3586 - aux_output_loss: 0.5158 - val_loss: 0.4263 - val_output_loss: 0.4093 - val_aux_output_loss: 0.5792\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3687 - output_loss: 0.3535 - aux_output_loss: 0.5053 - val_loss: 0.3911 - val_output_loss: 0.3751 - val_aux_output_loss: 0.5353\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20, validation_data=([X_val_A, X_val_B],[y_val, y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3726 - output_loss: 0.3591 - aux_output_loss: 0.4943\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9ceded34c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.46530092],\n",
       "        [1.6191185 ],\n",
       "        [3.2599525 ]], dtype=float32),\n",
       " array([[0.7211354],\n",
       "        [1.8221537],\n",
       "        [2.655029 ]], dtype=float32))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n",
    "y_pred_main, y_pred_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Subclass API to Build Dynamic Models\n",
    "Both the Sequential and the Functional API are declarative; you declare the layers and how they are connected, and then feed the model some data for training. But it's static. Some model involves loops, varying shapes,conditional branching, and other dynamic behaviour. For such cases, Subclassing API is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        inputA, inputB = inputs\n",
    "        hidden1 = self.hidden1(inputB)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([inputA, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(inputA)\n",
    "        \n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.0332 - output_1_loss: 0.7923 - output_2_loss: 3.2018 - val_loss: 0.6765 - val_output_1_loss: 0.5512 - val_output_2_loss: 1.8043\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5921 - output_1_loss: 0.5190 - output_2_loss: 1.2501 - val_loss: 0.5672 - val_output_1_loss: 0.5265 - val_output_2_loss: 0.9334\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5116 - output_1_loss: 0.4794 - output_2_loss: 0.8016 - val_loss: 0.5667 - val_output_1_loss: 0.5479 - val_output_2_loss: 0.7362\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4895 - output_1_loss: 0.4670 - output_2_loss: 0.6923 - val_loss: 0.4934 - val_output_1_loss: 0.4716 - val_output_2_loss: 0.6894\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4750 - output_1_loss: 0.4543 - output_2_loss: 0.6620 - val_loss: 0.4815 - val_output_1_loss: 0.4599 - val_output_2_loss: 0.6758\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4604 - output_1_loss: 0.4393 - output_2_loss: 0.6505 - val_loss: 0.4822 - val_output_1_loss: 0.4614 - val_output_2_loss: 0.6694\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4517 - output_1_loss: 0.4304 - output_2_loss: 0.6439 - val_loss: 0.4663 - val_output_1_loss: 0.4442 - val_output_2_loss: 0.6646\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4437 - output_1_loss: 0.4221 - output_2_loss: 0.6390 - val_loss: 0.4678 - val_output_1_loss: 0.4463 - val_output_2_loss: 0.6614\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4354 - output_1_loss: 0.4133 - output_2_loss: 0.6346 - val_loss: 0.4535 - val_output_1_loss: 0.4308 - val_output_2_loss: 0.6584\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4308 - output_1_loss: 0.4085 - output_2_loss: 0.6310 - val_loss: 0.4475 - val_output_1_loss: 0.4244 - val_output_2_loss: 0.6556\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4253 - output_1_loss: 0.4028 - output_2_loss: 0.6278 - val_loss: 0.4444 - val_output_1_loss: 0.4212 - val_output_2_loss: 0.6534\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4185 - output_1_loss: 0.3955 - output_2_loss: 0.6249 - val_loss: 0.4380 - val_output_1_loss: 0.4143 - val_output_2_loss: 0.6515\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4131 - output_1_loss: 0.3898 - output_2_loss: 0.6224 - val_loss: 0.4321 - val_output_1_loss: 0.4078 - val_output_2_loss: 0.6510\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4065 - output_1_loss: 0.3827 - output_2_loss: 0.6202 - val_loss: 0.4315 - val_output_1_loss: 0.4074 - val_output_2_loss: 0.6486\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4026 - output_1_loss: 0.3787 - output_2_loss: 0.6182 - val_loss: 0.4305 - val_output_1_loss: 0.4064 - val_output_2_loss: 0.6468\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4013 - output_1_loss: 0.3774 - output_2_loss: 0.6165 - val_loss: 0.4323 - val_output_1_loss: 0.4085 - val_output_2_loss: 0.6464\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3926 - output_1_loss: 0.3679 - output_2_loss: 0.6150 - val_loss: 0.4374 - val_output_1_loss: 0.4142 - val_output_2_loss: 0.6458\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3891 - output_1_loss: 0.3641 - output_2_loss: 0.6136 - val_loss: 0.4117 - val_output_1_loss: 0.3858 - val_output_2_loss: 0.6454\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3842 - output_1_loss: 0.3588 - output_2_loss: 0.6124 - val_loss: 0.4065 - val_output_1_loss: 0.3800 - val_output_2_loss: 0.6451\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3812 - output_1_loss: 0.3556 - output_2_loss: 0.6113 - val_loss: 0.4277 - val_output_1_loss: 0.4038 - val_output_2_loss: 0.6427\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20, validation_data=([X_val_A, X_val_B],[y_val, y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3979 - output_1_loss: 0.3732 - output_2_loss: 0.6206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.3249455],\n",
       "        [1.5949948],\n",
       "        [3.3147027]], dtype=float32),\n",
       " array([[1.0403594],\n",
       "        [1.5478688],\n",
       "        [2.5467997]], dtype=float32))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))\n",
    "\n",
    "y_pred_main, y_pred_aux "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Restoring a Model\n",
    "Saving a trained model can be done for Sequential and Functional API. Keras will use HDF5 format to save both the model architecture and the values of all the model parameters for every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideAndDeepModel(30, activation=\"relu\")\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.8688 - val_loss: 0.7796\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7040 - val_loss: 0.6453\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6180 - val_loss: 0.6001\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5762 - val_loss: 0.5717\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5496 - val_loss: 0.5476\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5267 - val_loss: 0.5303\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5100 - val_loss: 0.5157\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4960 - val_loss: 0.5045\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4856 - val_loss: 0.4955\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4771 - val_loss: 0.4883\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4842\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9ce9fc29d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.70360875],\n",
       "       [1.577575  ],\n",
       "       [2.8617046 ]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9cea6660d0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training may last for several hours, especially when training on large datasets. In this case, you should not only save your model at the end of the training, but also save checkpoints at regular intervals. \n",
    "\n",
    "This can be done using *Callbacks*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Callbacks during Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "357/363 [============================>.] - ETA: 0s - loss: 1.8856WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.8688\n",
      "Epoch 2/20\n",
      "326/363 [=========================>....] - ETA: 0s - loss: 0.7102WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7040\n",
      "Epoch 3/20\n",
      "347/363 [===========================>..] - ETA: 0s - loss: 0.6195WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6180\n",
      "Epoch 4/20\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.5766WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5762\n",
      "Epoch 5/20\n",
      "320/363 [=========================>....] - ETA: 0s - loss: 0.5511WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5496\n",
      "Epoch 6/20\n",
      "333/363 [==========================>...] - ETA: 0s - loss: 0.5352WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5267\n",
      "Epoch 7/20\n",
      "328/363 [==========================>...] - ETA: 0s - loss: 0.5074WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5100\n",
      "Epoch 8/20\n",
      "324/363 [=========================>....] - ETA: 0s - loss: 0.4962WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 959us/step - loss: 0.4960\n",
      "Epoch 9/20\n",
      "327/363 [==========================>...] - ETA: 0s - loss: 0.4866WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4856\n",
      "Epoch 10/20\n",
      "320/363 [=========================>....] - ETA: 0s - loss: 0.4765WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.4771\n",
      "Epoch 11/20\n",
      "328/363 [==========================>...] - ETA: 0s - loss: 0.4653WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 931us/step - loss: 0.4698\n",
      "Epoch 12/20\n",
      "328/363 [==========================>...] - ETA: 0s - loss: 0.4651WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4642\n",
      "Epoch 13/20\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4593WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4593\n",
      "Epoch 14/20\n",
      "334/363 [==========================>...] - ETA: 0s - loss: 0.4629WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 916us/step - loss: 0.4553\n",
      "Epoch 15/20\n",
      "344/363 [===========================>..] - ETA: 0s - loss: 0.4481WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4515\n",
      "Epoch 16/20\n",
      "346/363 [===========================>..] - ETA: 0s - loss: 0.4479WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4485\n",
      "Epoch 17/20\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.4448WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4454\n",
      "Epoch 18/20\n",
      "361/363 [============================>.] - ETA: 0s - loss: 0.4435WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 993us/step - loss: 0.4434\n",
      "Epoch 19/20\n",
      "333/363 [==========================>...] - ETA: 0s - loss: 0.4435WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 917us/step - loss: 0.4408\n",
      "Epoch 20/20\n",
      "348/363 [===========================>..] - ETA: 0s - loss: 0.4339WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4384\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_keras_model.h5', save_best_only=True) # save_best_only when using validation checkpoint\n",
    "history = model.fit(X_train, y_train, epochs=20, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 887us/step - loss: 0.3691\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping\n",
    "Use EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epochs(defined by the patience argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.3870\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.3765\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3779\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3791\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3755\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3768\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3759\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3757\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3747\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3743\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3741\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3723\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3454 - val_loss: 0.3732\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3443 - val_loss: 0.3729\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3437 - val_loss: 0.3746\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3432 - val_loss: 0.3709\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3436 - val_loss: 0.3728\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3420 - val_loss: 0.3698\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.3713\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3409 - val_loss: 0.3711\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3412 - val_loss: 0.3737\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3409 - val_loss: 0.3701\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3397 - val_loss: 0.3713\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3419 - val_loss: 0.3673\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3403 - val_loss: 0.3756\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3461 - val_loss: 0.3708\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3411 - val_loss: 0.3797\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3526 - val_loss: 0.3693\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3438 - val_loss: 0.3832\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3711\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3432 - val_loss: 0.3876\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3432 - val_loss: 0.3691\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3456 - val_loss: 0.3714\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3357 - val_loss: 0.3655\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.3640\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3342 - val_loss: 0.3638\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.3661\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3338 - val_loss: 0.3630\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3328 - val_loss: 0.3681\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.3648\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3327 - val_loss: 0.3701\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3334 - val_loss: 0.3633\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3314 - val_loss: 0.3666\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3316 - val_loss: 0.3600\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3304 - val_loss: 0.3627\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3303 - val_loss: 0.3602\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3295 - val_loss: 0.3622\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.3607\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3290 - val_loss: 0.3649\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.3604\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3279 - val_loss: 0.3592\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3273 - val_loss: 0.3612\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3269 - val_loss: 0.3581\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3264 - val_loss: 0.3584\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3259 - val_loss: 0.3587\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.3577\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3250 - val_loss: 0.3577\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3249 - val_loss: 0.3558\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3237 - val_loss: 0.3547\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.3614\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.3565\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3256 - val_loss: 0.3713\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3273 - val_loss: 0.3536\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3244 - val_loss: 0.3643\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3245 - val_loss: 0.3539\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3241 - val_loss: 0.3539\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3210 - val_loss: 0.3539\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3229 - val_loss: 0.3560\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3204 - val_loss: 0.3589\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3204 - val_loss: 0.3576\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3197 - val_loss: 0.3522\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3201 - val_loss: 0.3567\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.3528\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3302 - val_loss: 0.3538\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3188 - val_loss: 0.3509\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3185 - val_loss: 0.3518\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3184 - val_loss: 0.3524\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3186 - val_loss: 0.3591\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3195 - val_loss: 0.3498\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3175 - val_loss: 0.3556\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3253 - val_loss: 0.3513\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3338 - val_loss: 0.3584\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3210 - val_loss: 0.3513\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3265 - val_loss: 0.3530\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3188 - val_loss: 0.3549\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3251 - val_loss: 0.3518\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3166 - val_loss: 0.3478\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3159 - val_loss: 0.3594\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.3514\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3190 - val_loss: 0.3621\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3306 - val_loss: 0.3511\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3201 - val_loss: 0.3655\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3206 - val_loss: 0.3512\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3175 - val_loss: 0.3576\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3293 - val_loss: 0.3508\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3184 - val_loss: 0.3703\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3409 - val_loss: 0.3555\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 926us/step - loss: 0.3332\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing own custom callbacks\n",
    "The following callback will display the ratio between the validation loss and the training loss during the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCB(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print('\\n val/train: {:2f}'.format(logs['val_loss']/logs['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/363 [=========================>....] - ETA: 0s - loss: 0.3174\n",
      " val/train: 1.130085\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.3579\n"
     ]
    }
   ],
   "source": [
    "val_train_ratio_cb = PrintValTrainRatioCB()\n",
    "history = model.fit(X_train, y_train, epochs=1, validation_data=(X_val, y_val), callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorboard for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_logs/run_2020_08_17-09_48_05'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    \n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/363 [..............................] - ETA: 0s - loss: 8.4863WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0337s). Check your callbacks.\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.8688 - val_loss: 0.7796\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7040 - val_loss: 0.6453\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6180 - val_loss: 0.6001\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5762 - val_loss: 0.5717\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5496 - val_loss: 0.5476\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5267 - val_loss: 0.5303\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5100 - val_loss: 0.5157\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4960 - val_loss: 0.5045\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4856 - val_loss: 0.4955\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4771 - val_loss: 0.4883\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4698 - val_loss: 0.4830\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4642 - val_loss: 0.4779\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4593 - val_loss: 0.4732\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4553 - val_loss: 0.4700\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4515 - val_loss: 0.4683\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4485 - val_loss: 0.4639\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4454 - val_loss: 0.4632\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4434 - val_loss: 0.4593\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4408 - val_loss: 0.4570\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4384 - val_loss: 0.4561\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4361 - val_loss: 0.4541\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4348 - val_loss: 0.4515\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4323 - val_loss: 0.4498\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4311 - val_loss: 0.4470\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4286 - val_loss: 0.4475\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4275 - val_loss: 0.4463\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4255 - val_loss: 0.4435\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4241 - val_loss: 0.4415\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4225 - val_loss: 0.4417\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4214 - val_loss: 0.4397\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "One simple way is to try many combinations of hyperparameters and see which one workds best on the validation set (K-fold cross validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential model for univariate regression\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    \n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.2787 - val_loss: 0.8670\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7448 - val_loss: 0.6481\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6078 - val_loss: 0.5705\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5314 - val_loss: 0.5489\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5078 - val_loss: 0.5313\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4924 - val_loss: 0.5150\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4818 - val_loss: 0.5047\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4718 - val_loss: 0.4930\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4642 - val_loss: 0.4849\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4581 - val_loss: 0.4787\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4520 - val_loss: 0.4736\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4474 - val_loss: 0.4690\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4431 - val_loss: 0.4674\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4397 - val_loss: 0.4607\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4356 - val_loss: 0.4600\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4334 - val_loss: 0.4541\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4298 - val_loss: 0.4550\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4274 - val_loss: 0.4508\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4248 - val_loss: 0.4469\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4222 - val_loss: 0.4474\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4200 - val_loss: 0.4420\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4182 - val_loss: 0.4399\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4150 - val_loss: 0.4428\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4137 - val_loss: 0.4358\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4115 - val_loss: 0.4357\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4100 - val_loss: 0.4355\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4082 - val_loss: 0.4341\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4067 - val_loss: 0.4304\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4050 - val_loss: 0.4302\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4031 - val_loss: 0.4279\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4014 - val_loss: 0.4292\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4005 - val_loss: 0.4294\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3980 - val_loss: 0.4273\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3965 - val_loss: 0.4259\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3957 - val_loss: 0.4236\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3947 - val_loss: 0.4233\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3924 - val_loss: 0.4215\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3920 - val_loss: 0.4184\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3900 - val_loss: 0.4216\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3891 - val_loss: 0.4198\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3874 - val_loss: 0.4177\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3865 - val_loss: 0.4173\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3843 - val_loss: 0.4146\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3839 - val_loss: 0.4141\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3827 - val_loss: 0.4142\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.4119\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3806 - val_loss: 0.4123\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.4136\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3781 - val_loss: 0.4101\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3775 - val_loss: 0.4104\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3754 - val_loss: 0.4099\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.4099\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3740 - val_loss: 0.4070\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3734 - val_loss: 0.4056\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3718 - val_loss: 0.4079\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3710 - val_loss: 0.4047\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 0.4048\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.4034\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3676 - val_loss: 0.4051\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3672 - val_loss: 0.4043\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3671 - val_loss: 0.4017\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.4057\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3651 - val_loss: 0.4000\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.4018\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.4003\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3625 - val_loss: 0.3999\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3627 - val_loss: 0.3995\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3611 - val_loss: 0.4011\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3607 - val_loss: 0.4041\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3601 - val_loss: 0.4045\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3598 - val_loss: 0.3972\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3590 - val_loss: 0.3979\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.3957\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3589 - val_loss: 0.3972\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3568 - val_loss: 0.3954\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3566 - val_loss: 0.3948\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3561 - val_loss: 0.3999\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3557 - val_loss: 0.3955\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3558 - val_loss: 0.3941\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3934\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.3962\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3546 - val_loss: 0.3975\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3531 - val_loss: 0.3945\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.3936\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.4058\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3518 - val_loss: 0.3923\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3504 - val_loss: 0.3933\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 0.3921\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3500 - val_loss: 0.3923\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3951\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3534 - val_loss: 0.3936\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.3943\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3912\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3893\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3933\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3959\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3887\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3873\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3879\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9cecc543a0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100, \n",
    "              validation_data=(X_val, y_val), \n",
    "            callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3583\n"
     ]
    }
   ],
   "source": [
    "mse_test = keras_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4993362, 1.5726289, 3.6899676], dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = keras_reg.predict(X_new)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are many hyperparameters, it is preferable to use randomized search rather than grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: the following cell crashes at the end of training. This seems to be caused by [Keras issue #13586](https://github.com/keras-team/keras/issues/13586), which was triggered by a recent change in Scikit-Learn. [Pull Request #13598](https://github.com/keras-team/keras/pull/13598) seems to fix the issue, so this problem should be resolved soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] learning_rate=0.0006665773642619745, n_hidden=0, n_neurons=36 ...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 5.4828 - val_loss: 4.0233\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1198 - val_loss: 2.3990\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9709 - val_loss: 1.6071\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3932 - val_loss: 1.2067\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0950 - val_loss: 0.9991\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9362 - val_loss: 0.8882\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8482 - val_loss: 0.8252\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7964 - val_loss: 0.7871\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7637 - val_loss: 0.7618\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7411 - val_loss: 0.7433\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7240 - val_loss: 0.7287\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7101 - val_loss: 0.7163\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6982 - val_loss: 0.7054\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6876 - val_loss: 0.6952\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6778 - val_loss: 0.6857\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6688 - val_loss: 0.6770\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6604 - val_loss: 0.6688\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6525 - val_loss: 0.6612\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6450 - val_loss: 0.6539\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6380 - val_loss: 0.6472\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6314 - val_loss: 0.6407\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6252 - val_loss: 0.6345\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6193 - val_loss: 0.6290\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6138 - val_loss: 0.6235\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6085 - val_loss: 0.6184\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6036 - val_loss: 0.6137\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5989 - val_loss: 0.6090\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5945 - val_loss: 0.6048\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5903 - val_loss: 0.6008\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5864 - val_loss: 0.5971\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5827 - val_loss: 0.5936\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5792 - val_loss: 0.5903\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5759 - val_loss: 0.5870\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5728 - val_loss: 0.5840\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5698 - val_loss: 0.5814\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5670 - val_loss: 0.5788\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5644 - val_loss: 0.5763\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5619 - val_loss: 0.5740\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5595 - val_loss: 0.5719\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5573 - val_loss: 0.5698\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5552 - val_loss: 0.5679\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5532 - val_loss: 0.5660\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5514 - val_loss: 0.5642\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5496 - val_loss: 0.5625\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5479 - val_loss: 0.5610\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5464 - val_loss: 0.5596\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5449 - val_loss: 0.5583\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5434 - val_loss: 0.5571\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5421 - val_loss: 0.5558\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5409 - val_loss: 0.5547\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5397 - val_loss: 0.5537\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5386 - val_loss: 0.5526\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5375 - val_loss: 0.5517\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5365 - val_loss: 0.5508\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5355 - val_loss: 0.5501\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5346 - val_loss: 0.5493\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5338 - val_loss: 0.5486\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5329 - val_loss: 0.5479\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5322 - val_loss: 0.5473\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5315 - val_loss: 0.5467\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5308 - val_loss: 0.5461\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5302 - val_loss: 0.5455\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5295 - val_loss: 0.5449\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5290 - val_loss: 0.5444\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 0.5440\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5279 - val_loss: 0.5436\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5274 - val_loss: 0.5432\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 0.5428\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5265 - val_loss: 0.5425\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5261 - val_loss: 0.5422\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5257 - val_loss: 0.5418\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5253 - val_loss: 0.5416\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 0.5413\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 0.5411\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 0.5408\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5240 - val_loss: 0.5406\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 0.5404\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 0.5402\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 0.5399\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5230 - val_loss: 0.5398\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5228 - val_loss: 0.5397\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5225 - val_loss: 0.5394\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5224 - val_loss: 0.5393\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5222 - val_loss: 0.5392\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.5391\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5218 - val_loss: 0.5389\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5216 - val_loss: 0.5388\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5215 - val_loss: 0.5387\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5213 - val_loss: 0.5386\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5212 - val_loss: 0.5386\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5211 - val_loss: 0.5385\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5209 - val_loss: 0.5384\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5208 - val_loss: 0.5383\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5207 - val_loss: 0.5382\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5206 - val_loss: 0.5380\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5205 - val_loss: 0.5380\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5203 - val_loss: 0.5379\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5203 - val_loss: 0.5379\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5202 - val_loss: 0.5378\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5201 - val_loss: 0.5378\n",
      "121/121 [==============================] - 0s 765us/step - loss: 0.5150\n",
      "[CV]  learning_rate=0.0006665773642619745, n_hidden=0, n_neurons=36, total=  34.2s\n",
      "[CV] learning_rate=0.0006665773642619745, n_hidden=0, n_neurons=36 ...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   34.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 6.1853 - val_loss: 4.2448\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4649 - val_loss: 2.6087\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1763 - val_loss: 1.7684\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5207 - val_loss: 1.3187\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1717 - val_loss: 1.0704\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9781 - val_loss: 0.9298\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8664 - val_loss: 0.8463\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7985 - val_loss: 0.7940\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7549 - val_loss: 0.7597\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7253 - val_loss: 0.7357\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7036 - val_loss: 0.7176\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6869 - val_loss: 0.7030\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6731 - val_loss: 0.6910\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6613 - val_loss: 0.6805\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6509 - val_loss: 0.6711\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6416 - val_loss: 0.6624\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6331 - val_loss: 0.6545\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6252 - val_loss: 0.6471\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6179 - val_loss: 0.6403\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6110 - val_loss: 0.6342\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6047 - val_loss: 0.6283\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5987 - val_loss: 0.6226\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5931 - val_loss: 0.6176\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5879 - val_loss: 0.6127\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5829 - val_loss: 0.6082\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5783 - val_loss: 0.6040\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5740 - val_loss: 0.5998\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5699 - val_loss: 0.5961\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5660 - val_loss: 0.5927\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5624 - val_loss: 0.5894\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5590 - val_loss: 0.5862\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5559 - val_loss: 0.5832\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5528 - val_loss: 0.5803\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5500 - val_loss: 0.5778\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5474 - val_loss: 0.5756\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5733\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5425 - val_loss: 0.5711\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5403 - val_loss: 0.5690\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5382 - val_loss: 0.5672\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5363 - val_loss: 0.5655\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5344 - val_loss: 0.5641\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5327 - val_loss: 0.5625\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.5609\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5296 - val_loss: 0.5594\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5282 - val_loss: 0.5582\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5268 - val_loss: 0.5568\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5255 - val_loss: 0.5558\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 0.5550\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 0.5539\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5221 - val_loss: 0.5528\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5211 - val_loss: 0.5521\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5202 - val_loss: 0.5512\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5193 - val_loss: 0.5505\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.5496\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5177 - val_loss: 0.5492\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5169 - val_loss: 0.5484\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5162 - val_loss: 0.5480\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5156 - val_loss: 0.5473\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5150 - val_loss: 0.5467\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5144 - val_loss: 0.5464\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5139 - val_loss: 0.5460\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5134 - val_loss: 0.5455\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5128 - val_loss: 0.5452\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5123 - val_loss: 0.5443\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5120 - val_loss: 0.5440\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5116 - val_loss: 0.5438\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5112 - val_loss: 0.5433\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5108 - val_loss: 0.5429\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5105 - val_loss: 0.5428\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5102 - val_loss: 0.5423\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5099 - val_loss: 0.5421\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5096 - val_loss: 0.5421\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5093 - val_loss: 0.5421\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5090 - val_loss: 0.5417\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5089 - val_loss: 0.5414\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5086 - val_loss: 0.5413\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5083 - val_loss: 0.5409\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5082 - val_loss: 0.5408\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5079 - val_loss: 0.5408\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5078 - val_loss: 0.5409\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5076 - val_loss: 0.5406\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5074 - val_loss: 0.5404\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5073 - val_loss: 0.5401\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5072 - val_loss: 0.5400\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5070 - val_loss: 0.5399\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5069 - val_loss: 0.5401\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5068 - val_loss: 0.5401\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5067 - val_loss: 0.5400\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5066 - val_loss: 0.5397\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5064 - val_loss: 0.5398\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5063 - val_loss: 0.5395\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5063 - val_loss: 0.5393\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5062 - val_loss: 0.5389\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5061 - val_loss: 0.5390\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5060 - val_loss: 0.5392\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5059 - val_loss: 0.5394\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5058 - val_loss: 0.5391\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5058 - val_loss: 0.5393\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5058 - val_loss: 0.5391\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5057 - val_loss: 0.5390\n",
      "121/121 [==============================] - 0s 770us/step - loss: 0.5433\n",
      "[CV]  learning_rate=0.0006665773642619745, n_hidden=0, n_neurons=36, total=  34.5s\n",
      "[CV] learning_rate=0.0006665773642619745, n_hidden=0, n_neurons=36 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4716 - val_loss: 4.4849\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7070 - val_loss: 2.7368\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3426 - val_loss: 1.8478\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6495 - val_loss: 1.3873\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2870 - val_loss: 1.1413\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0909 - val_loss: 1.0058\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9795 - val_loss: 0.9267\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9117 - val_loss: 0.8763\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8672 - val_loss: 0.8421\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8356 - val_loss: 0.8167\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8111 - val_loss: 0.7962\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7907 - val_loss: 0.7782\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7732 - val_loss: 0.7626\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7574 - val_loss: 0.7482\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7430 - val_loss: 0.7352\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7297 - val_loss: 0.7225\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7174 - val_loss: 0.7108\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7058 - val_loss: 0.6996\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6950 - val_loss: 0.6895\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6848 - val_loss: 0.6800\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6754 - val_loss: 0.6712\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6664 - val_loss: 0.6628\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6580 - val_loss: 0.6549\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6500 - val_loss: 0.6477\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6426 - val_loss: 0.6407\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6357 - val_loss: 0.6341\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6290 - val_loss: 0.6280\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6229 - val_loss: 0.6222\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6170 - val_loss: 0.6169\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6116 - val_loss: 0.6118\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6064 - val_loss: 0.6071\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6016 - val_loss: 0.6026\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5970 - val_loss: 0.5982\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5928 - val_loss: 0.5944\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5887 - val_loss: 0.5908\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5849 - val_loss: 0.5874\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5814 - val_loss: 0.5843\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5780 - val_loss: 0.5813\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5749 - val_loss: 0.5785\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5719 - val_loss: 0.5757\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5691 - val_loss: 0.5733\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5664 - val_loss: 0.5710\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5640 - val_loss: 0.5687\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5616 - val_loss: 0.5668\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5595 - val_loss: 0.5648\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5574 - val_loss: 0.5629\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5555 - val_loss: 0.5613\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5536 - val_loss: 0.5597\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5519 - val_loss: 0.5584\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5503 - val_loss: 0.5570\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5488 - val_loss: 0.5557\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5474 - val_loss: 0.5545\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5460 - val_loss: 0.5533\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5447 - val_loss: 0.5522\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5436 - val_loss: 0.5513\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5424 - val_loss: 0.5502\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5414 - val_loss: 0.5495\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5404 - val_loss: 0.5486\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5394 - val_loss: 0.5479\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5386 - val_loss: 0.5472\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5377 - val_loss: 0.5466\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5369 - val_loss: 0.5461\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5362 - val_loss: 0.5455\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5355 - val_loss: 0.5448\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5349 - val_loss: 0.5441\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5343 - val_loss: 0.5437\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5337 - val_loss: 0.5433\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5331 - val_loss: 0.5428\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5326 - val_loss: 0.5424\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5321 - val_loss: 0.5420\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5317 - val_loss: 0.5417\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.5415\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5308 - val_loss: 0.5413\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.5410\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.5407\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5298 - val_loss: 0.5406\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5294 - val_loss: 0.5402\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5291 - val_loss: 0.5401\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5288 - val_loss: 0.5399\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5286 - val_loss: 0.5398\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5283 - val_loss: 0.5395\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5281 - val_loss: 0.5394\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5279 - val_loss: 0.5392\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5277 - val_loss: 0.5391\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5275 - val_loss: 0.5390\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5273 - val_loss: 0.5389\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5271 - val_loss: 0.5387\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5269 - val_loss: 0.5386\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5267 - val_loss: 0.5386\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5266 - val_loss: 0.5385\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5265 - val_loss: 0.5384\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5263 - val_loss: 0.5383\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5262 - val_loss: 0.5380\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5261 - val_loss: 0.5381\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5260 - val_loss: 0.5382\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5259 - val_loss: 0.5383\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5258 - val_loss: 0.5383\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5257 - val_loss: 0.5382\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5256 - val_loss: 0.5382\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5255 - val_loss: 0.5381\n",
      "121/121 [==============================] - 0s 765us/step - loss: 0.5016\n",
      "[CV]  learning_rate=0.0006665773642619745, n_hidden=0, n_neurons=36, total=  31.8s\n",
      "[CV] learning_rate=0.0006943739671561459, n_hidden=3, n_neurons=2 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.4628 - val_loss: 3.5422\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9258 - val_loss: 2.4688\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1412 - val_loss: 1.9213\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7411 - val_loss: 1.6424\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5371 - val_loss: 1.5005\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4330 - val_loss: 1.4282\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3800 - val_loss: 1.3915\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3528 - val_loss: 1.3727\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3390 - val_loss: 1.3633\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3320 - val_loss: 1.3585\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3283 - val_loss: 1.3561\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3265 - val_loss: 1.3549\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3255 - val_loss: 1.3543\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3251 - val_loss: 1.3540\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3248 - val_loss: 1.3539\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3247 - val_loss: 1.3538\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3247 - val_loss: 1.3538\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3246 - val_loss: 1.3538\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 1.3345\n",
      "[CV]  learning_rate=0.0006943739671561459, n_hidden=3, n_neurons=2, total=  12.9s\n",
      "[CV] learning_rate=0.0006943739671561459, n_hidden=3, n_neurons=2 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.2649 - val_loss: 3.4168\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.7322 - val_loss: 2.3416\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.0076 - val_loss: 1.8673\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.7062 - val_loss: 1.6708\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 1.5774 - val_loss: 1.5782\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.5100 - val_loss: 1.5248\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4672 - val_loss: 1.4887\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4369 - val_loss: 1.4627\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4147 - val_loss: 1.4435\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3978 - val_loss: 1.4289\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.3846 - val_loss: 1.4173\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3742 - val_loss: 1.4083\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3658 - val_loss: 1.4008\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3590 - val_loss: 1.3948\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3534 - val_loss: 1.3898\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3487 - val_loss: 1.3856\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3448 - val_loss: 1.3821\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3414 - val_loss: 1.3791\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3386 - val_loss: 1.3764\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3361 - val_loss: 1.3743\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3340 - val_loss: 1.3723\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3322 - val_loss: 1.3706\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3305 - val_loss: 1.3691\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3291 - val_loss: 1.3678\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3278 - val_loss: 1.3667\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3267 - val_loss: 1.3656\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3257 - val_loss: 1.3647\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3247 - val_loss: 1.3638\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3239 - val_loss: 1.3631\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3232 - val_loss: 1.3624\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3225 - val_loss: 1.3618\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3219 - val_loss: 1.3613\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3214 - val_loss: 1.3609\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3209 - val_loss: 1.3605\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3204 - val_loss: 1.3600\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3200 - val_loss: 1.3596\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3196 - val_loss: 1.3592\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3193 - val_loss: 1.3589\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3190 - val_loss: 1.3586\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3187 - val_loss: 1.3584\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3184 - val_loss: 1.3581\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3181 - val_loss: 1.3579\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3179 - val_loss: 1.3577\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3177 - val_loss: 1.3575\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3175 - val_loss: 1.3573\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3173 - val_loss: 1.3572\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3171 - val_loss: 1.3570\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3169 - val_loss: 1.3569\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3168 - val_loss: 1.3568\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3166 - val_loss: 1.3567\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3165 - val_loss: 1.3566\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3164 - val_loss: 1.3564\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3162 - val_loss: 1.3563\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3161 - val_loss: 1.3562\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3160 - val_loss: 1.3561\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3158 - val_loss: 1.3561\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3158 - val_loss: 1.3560\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3157 - val_loss: 1.3559\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3156 - val_loss: 1.3558\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3155 - val_loss: 1.3557\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3154 - val_loss: 1.3557\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3153 - val_loss: 1.3555\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3153 - val_loss: 1.3555\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3152 - val_loss: 1.3555\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3151 - val_loss: 1.3554\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3151 - val_loss: 1.3554\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3150 - val_loss: 1.3553\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3149 - val_loss: 1.3553\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3149 - val_loss: 1.3552\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3148 - val_loss: 1.3551\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3148 - val_loss: 1.3550\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3147 - val_loss: 1.3550\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3147 - val_loss: 1.3550\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3146 - val_loss: 1.3550\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3146 - val_loss: 1.3550\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3145 - val_loss: 1.3550\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3145 - val_loss: 1.3549\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3144 - val_loss: 1.3548\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3144 - val_loss: 1.3547\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3144 - val_loss: 1.3547\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3143 - val_loss: 1.3547\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3143 - val_loss: 1.3547\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3143 - val_loss: 1.3546\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3142 - val_loss: 1.3545\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3142 - val_loss: 1.3545\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3142 - val_loss: 1.3545\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3141 - val_loss: 1.3545\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3141 - val_loss: 1.3544\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3141 - val_loss: 1.3544\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3140 - val_loss: 1.3544\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3140 - val_loss: 1.3543\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3140 - val_loss: 1.3543\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3140 - val_loss: 1.3543\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3139 - val_loss: 1.3543\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3139 - val_loss: 1.3543\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3139 - val_loss: 1.3542\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3138 - val_loss: 1.3542\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3138 - val_loss: 1.3542\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3138 - val_loss: 1.3542\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3138 - val_loss: 1.3542\n",
      "121/121 [==============================] - 0s 805us/step - loss: 1.3573\n",
      "[CV]  learning_rate=0.0006943739671561459, n_hidden=3, n_neurons=2, total=  36.9s\n",
      "[CV] learning_rate=0.0006943739671561459, n_hidden=3, n_neurons=2 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.8851 - val_loss: 2.7775\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3120 - val_loss: 1.9395\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7677 - val_loss: 1.6257\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5611 - val_loss: 1.4956\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4736 - val_loss: 1.4350\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4303 - val_loss: 1.4051\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4035 - val_loss: 1.3854\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3835 - val_loss: 1.3715\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3711 - val_loss: 1.3633\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3623 - val_loss: 1.3578\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3546 - val_loss: 1.3537\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3487 - val_loss: 1.3507\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3444 - val_loss: 1.3482\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3403 - val_loss: 1.3462\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3368 - val_loss: 1.3442\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3331 - val_loss: 1.3420\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3298 - val_loss: 1.3393\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3257 - val_loss: 1.3369\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3222 - val_loss: 1.3336\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3176 - val_loss: 1.3308\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3132 - val_loss: 1.3270\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3082 - val_loss: 1.3226\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3023 - val_loss: 1.3177\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2953 - val_loss: 1.3126\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2880 - val_loss: 1.3061\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2801 - val_loss: 1.2980\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2710 - val_loss: 1.2893\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2618 - val_loss: 1.2800\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2518 - val_loss: 1.2696\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2410 - val_loss: 1.2584\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2292 - val_loss: 1.2460\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2169 - val_loss: 1.2324\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2023 - val_loss: 1.2172\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1875 - val_loss: 1.2017\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1721 - val_loss: 1.1849\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1560 - val_loss: 1.1678\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1398 - val_loss: 1.1500\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1229 - val_loss: 1.1318\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1061 - val_loss: 1.1141\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0893 - val_loss: 1.0961\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0723 - val_loss: 1.0784\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0555 - val_loss: 1.0611\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0394 - val_loss: 1.0446\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0237 - val_loss: 1.0301\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0092 - val_loss: 1.0163\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9957 - val_loss: 1.0037\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9829 - val_loss: 0.9922\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9709 - val_loss: 0.9807\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9592 - val_loss: 0.9700\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9483 - val_loss: 0.9601\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9381 - val_loss: 0.9506\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9283 - val_loss: 0.9412\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9189 - val_loss: 0.9320\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9099 - val_loss: 0.9233\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9012 - val_loss: 0.9146\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8922 - val_loss: 0.9054\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8835 - val_loss: 0.8972\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8749 - val_loss: 0.8874\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8663 - val_loss: 0.8787\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8577 - val_loss: 0.8703\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8490 - val_loss: 0.8617\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8403 - val_loss: 0.8537\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8320 - val_loss: 0.8453\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8234 - val_loss: 0.8368\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8148 - val_loss: 0.8284\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8062 - val_loss: 0.8201\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7978 - val_loss: 0.8119\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7896 - val_loss: 0.8041\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7813 - val_loss: 0.7961\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7732 - val_loss: 0.7885\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7652 - val_loss: 0.7809\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7570 - val_loss: 0.7735\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7482 - val_loss: 0.7649\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7391 - val_loss: 0.7570\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7302 - val_loss: 0.7492\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7214 - val_loss: 0.7413\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7128 - val_loss: 0.7332\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7039 - val_loss: 0.7247\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6948 - val_loss: 0.7156\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6856 - val_loss: 0.7064\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6767 - val_loss: 0.6969\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6680 - val_loss: 0.6877\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6591 - val_loss: 0.6783\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6498 - val_loss: 0.6679\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6401 - val_loss: 0.6581\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6301 - val_loss: 0.6482\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6204 - val_loss: 0.6388\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6112 - val_loss: 0.6307\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6024 - val_loss: 0.6217\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5943 - val_loss: 0.6131\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5863 - val_loss: 0.6052\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5787 - val_loss: 0.5978\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5717 - val_loss: 0.5903\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5651 - val_loss: 0.5833\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5592 - val_loss: 0.5769\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5537 - val_loss: 0.5721\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5486 - val_loss: 0.5675\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5439 - val_loss: 0.5611\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5394 - val_loss: 0.5557\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5345 - val_loss: 0.5507\n",
      "121/121 [==============================] - 0s 801us/step - loss: 0.5188\n",
      "[CV]  learning_rate=0.0006943739671561459, n_hidden=3, n_neurons=2, total=  34.9s\n",
      "[CV] learning_rate=0.002125362451575068, n_hidden=1, n_neurons=4 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.6132 - val_loss: 1.3266\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8961 - val_loss: 0.7389\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6647 - val_loss: 0.6409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5966 - val_loss: 0.5888\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5534 - val_loss: 0.5542\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 0.5306\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5038 - val_loss: 0.5153\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4908 - val_loss: 0.5054\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4819 - val_loss: 0.4970\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4751 - val_loss: 0.4913\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4696 - val_loss: 0.4843\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4657 - val_loss: 0.4807\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4625 - val_loss: 0.4782\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4598 - val_loss: 0.4740\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4579 - val_loss: 0.4736\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4561 - val_loss: 0.4726\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4544 - val_loss: 0.4705\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4532 - val_loss: 0.4702\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4518 - val_loss: 0.4710\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4508 - val_loss: 0.4667\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4493 - val_loss: 0.4651\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4482 - val_loss: 0.4671\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4475 - val_loss: 0.4638\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4464 - val_loss: 0.4648\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4457 - val_loss: 0.4643\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4449 - val_loss: 0.4595\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4438 - val_loss: 0.4593\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4430 - val_loss: 0.4601\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4416 - val_loss: 0.4596\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4414 - val_loss: 0.4561\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4405 - val_loss: 0.4569\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4397 - val_loss: 0.4552\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4389 - val_loss: 0.4550\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4379 - val_loss: 0.4550\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4370 - val_loss: 0.4537\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4361 - val_loss: 0.4535\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4352 - val_loss: 0.4543\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4345 - val_loss: 0.4531\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4512\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4329 - val_loss: 0.4499\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4319 - val_loss: 0.4490\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4311 - val_loss: 0.4506\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4477\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4297 - val_loss: 0.4504\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4290 - val_loss: 0.4473\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4283 - val_loss: 0.4493\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4282 - val_loss: 0.4461\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4270 - val_loss: 0.4458\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4271 - val_loss: 0.4440\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4262 - val_loss: 0.4448\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4252 - val_loss: 0.4430\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4255 - val_loss: 0.4439\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4247 - val_loss: 0.4424\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4242 - val_loss: 0.4441\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4235 - val_loss: 0.4430\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4232 - val_loss: 0.4423\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4228 - val_loss: 0.4431\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4226 - val_loss: 0.4417\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4221 - val_loss: 0.4412\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4218 - val_loss: 0.4406\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4208 - val_loss: 0.4389\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4209 - val_loss: 0.4399\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4204 - val_loss: 0.4382\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.4399\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.4399\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.4385\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.4382\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4189 - val_loss: 0.4384\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4183 - val_loss: 0.4381\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4179 - val_loss: 0.4372\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4172 - val_loss: 0.4404\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4171 - val_loss: 0.4373\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4165 - val_loss: 0.4369\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4166 - val_loss: 0.4376\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4162 - val_loss: 0.4382\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.4372\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.4359\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4156 - val_loss: 0.4355\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.4344\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4150 - val_loss: 0.4362\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4145 - val_loss: 0.4365\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4141 - val_loss: 0.4362\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4139 - val_loss: 0.4360\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.4349\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4134 - val_loss: 0.4365\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4124 - val_loss: 0.4341\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4127 - val_loss: 0.4358\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4127 - val_loss: 0.4335\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4127 - val_loss: 0.4327\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.4361\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4123 - val_loss: 0.4336\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4115 - val_loss: 0.4362\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4121 - val_loss: 0.4345\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4110 - val_loss: 0.4335\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4109 - val_loss: 0.4351\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4110 - val_loss: 0.4325\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4102 - val_loss: 0.4346\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4106 - val_loss: 0.4337\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4101 - val_loss: 0.4326\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4098 - val_loss: 0.4328\n",
      "121/121 [==============================] - 0s 789us/step - loss: 0.3955\n",
      "[CV]  learning_rate=0.002125362451575068, n_hidden=1, n_neurons=4, total=  33.5s\n",
      "[CV] learning_rate=0.002125362451575068, n_hidden=1, n_neurons=4 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.9151 - val_loss: 1.0720\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8881 - val_loss: 0.8262\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7713 - val_loss: 0.7509\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7103 - val_loss: 0.7050\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6664 - val_loss: 0.6695\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6306 - val_loss: 0.6393\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6001 - val_loss: 0.6131\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5735 - val_loss: 0.5910\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5504 - val_loss: 0.5711\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5308 - val_loss: 0.5546\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5148 - val_loss: 0.5424\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5024 - val_loss: 0.5326\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4926 - val_loss: 0.5264\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4846 - val_loss: 0.5208\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4780 - val_loss: 0.5140\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4735 - val_loss: 0.5117\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4691 - val_loss: 0.5096\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4655 - val_loss: 0.5069\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4624 - val_loss: 0.5048\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4596 - val_loss: 0.5022\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4575 - val_loss: 0.4995\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4548 - val_loss: 0.4970\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4530 - val_loss: 0.4955\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4509 - val_loss: 0.4940\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4493 - val_loss: 0.4923\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4474 - val_loss: 0.4917\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4462 - val_loss: 0.4896\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4444 - val_loss: 0.4887\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4432 - val_loss: 0.4851\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4410 - val_loss: 0.4809\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4389 - val_loss: 0.4793\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4373 - val_loss: 0.4767\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4359 - val_loss: 0.4763\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4352 - val_loss: 0.4756\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4343 - val_loss: 0.4751\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4335 - val_loss: 0.4743\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4326 - val_loss: 0.4746\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4319 - val_loss: 0.4723\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4314 - val_loss: 0.4718\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4712\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4297 - val_loss: 0.4721\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.4719\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4286 - val_loss: 0.4704\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4278 - val_loss: 0.4711\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4271 - val_loss: 0.4688\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.4679\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.4695\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4253 - val_loss: 0.4674\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.4664\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4240 - val_loss: 0.4666\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4234 - val_loss: 0.4654\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.4660\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4217 - val_loss: 0.4643\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.4659\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4207 - val_loss: 0.4656\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.4647\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4197 - val_loss: 0.4643\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.4635\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4186 - val_loss: 0.4623\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4178 - val_loss: 0.4610\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4172 - val_loss: 0.4605\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4164 - val_loss: 0.4627\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.4594\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4154 - val_loss: 0.4601\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4150 - val_loss: 0.4593\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4143 - val_loss: 0.4579\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4138 - val_loss: 0.4573\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.4572\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.4578\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4124 - val_loss: 0.4552\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.4557\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4113 - val_loss: 0.4550\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.4541\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4105 - val_loss: 0.4535\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4100 - val_loss: 0.4538\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.4534\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4091 - val_loss: 0.4520\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4085 - val_loss: 0.4511\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.4508\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.4514\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4074 - val_loss: 0.4506\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4071 - val_loss: 0.4501\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.4500\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4064 - val_loss: 0.4491\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4062 - val_loss: 0.4482\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4053 - val_loss: 0.4476\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4051 - val_loss: 0.4498\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4048 - val_loss: 0.4484\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4044 - val_loss: 0.4462\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.4469\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4037 - val_loss: 0.4474\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4035 - val_loss: 0.4462\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4031 - val_loss: 0.4449\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4030 - val_loss: 0.4463\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4029 - val_loss: 0.4445\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4022 - val_loss: 0.4435\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4021 - val_loss: 0.4436\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4016 - val_loss: 0.4469\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4018 - val_loss: 0.4425\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4012 - val_loss: 0.4441\n",
      "121/121 [==============================] - 0s 795us/step - loss: 0.4411\n",
      "[CV]  learning_rate=0.002125362451575068, n_hidden=1, n_neurons=4, total=  33.5s\n",
      "[CV] learning_rate=0.002125362451575068, n_hidden=1, n_neurons=4 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.9686 - val_loss: 1.0700\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8701 - val_loss: 0.7758\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7379 - val_loss: 0.7343\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7017 - val_loss: 0.7156\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6785 - val_loss: 0.6978\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6604 - val_loss: 0.6837\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6447 - val_loss: 0.6743\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6306 - val_loss: 0.6547\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6172 - val_loss: 0.6436\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6041 - val_loss: 0.6314\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5927 - val_loss: 0.6226\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5816 - val_loss: 0.6081\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5709 - val_loss: 0.5949\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5615 - val_loss: 0.5895\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5524 - val_loss: 0.5807\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5441 - val_loss: 0.5828\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5368 - val_loss: 0.5632\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5299 - val_loss: 0.5594\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 0.5512\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5179 - val_loss: 0.5651\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5136 - val_loss: 0.5449\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5090 - val_loss: 0.5473\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5047 - val_loss: 0.5326\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5012 - val_loss: 0.5353\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4980 - val_loss: 0.5254\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4947 - val_loss: 0.5337\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4921 - val_loss: 0.5220\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4891 - val_loss: 0.5306\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4872 - val_loss: 0.5254\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4845 - val_loss: 0.5192\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4823 - val_loss: 0.5095\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4805 - val_loss: 0.5201\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4782 - val_loss: 0.5019\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4770 - val_loss: 0.5086\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4750 - val_loss: 0.5017\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4732 - val_loss: 0.5062\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4718 - val_loss: 0.5012\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4697 - val_loss: 0.5023\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4679 - val_loss: 0.4984\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4671 - val_loss: 0.4928\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4654 - val_loss: 0.4982\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4641 - val_loss: 0.4982\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4626 - val_loss: 0.4886\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4614 - val_loss: 0.4889\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4597 - val_loss: 0.5006\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4580 - val_loss: 0.4839\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.4958\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4564 - val_loss: 0.4951\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4556 - val_loss: 0.4818\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4551 - val_loss: 0.4831\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4535 - val_loss: 0.4818\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4532 - val_loss: 0.4857\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4518 - val_loss: 0.4804\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4507 - val_loss: 0.4793\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4498 - val_loss: 0.4828\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4487 - val_loss: 0.4741\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4477 - val_loss: 0.4802\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4467 - val_loss: 0.4744\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4462 - val_loss: 0.4809\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4452 - val_loss: 0.4720\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4440 - val_loss: 0.4739\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4428 - val_loss: 0.4896\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4428 - val_loss: 0.4784\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4418 - val_loss: 0.4703\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4410 - val_loss: 0.4676\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4401 - val_loss: 0.4682\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4394 - val_loss: 0.4668\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4388 - val_loss: 0.4674\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4375 - val_loss: 0.4644\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4366 - val_loss: 0.4610\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4365 - val_loss: 0.4653\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4353 - val_loss: 0.4635\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4344 - val_loss: 0.4765\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4334 - val_loss: 0.4696\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4331 - val_loss: 0.4620\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4317 - val_loss: 0.4744\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4317 - val_loss: 0.4608\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4664\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.4704\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4295 - val_loss: 0.4601\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4286 - val_loss: 0.4582\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4278 - val_loss: 0.4611\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4268 - val_loss: 0.4564\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4266 - val_loss: 0.4594\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.4582\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4252 - val_loss: 0.4588\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4237 - val_loss: 0.4517\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.4532\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4223 - val_loss: 0.4620\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4223 - val_loss: 0.4546\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4214 - val_loss: 0.4583\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4211 - val_loss: 0.4523\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4202 - val_loss: 0.4508\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4197 - val_loss: 0.4598\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4194 - val_loss: 0.4559\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4182 - val_loss: 0.4621\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4457\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4173 - val_loss: 0.4538\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4167 - val_loss: 0.4525\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.4608\n",
      "121/121 [==============================] - 0s 769us/step - loss: 0.4199\n",
      "[CV]  learning_rate=0.002125362451575068, n_hidden=1, n_neurons=4, total=  34.4s\n",
      "[CV] learning_rate=0.0007027974317461211, n_hidden=2, n_neurons=18 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.9680 - val_loss: 1.8893\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4773 - val_loss: 1.2667\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0447 - val_loss: 0.9593\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8250 - val_loss: 0.8031\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7163 - val_loss: 0.7320\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6642 - val_loss: 0.6933\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6332 - val_loss: 0.6654\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6097 - val_loss: 0.6423\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5903 - val_loss: 0.6227\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5737 - val_loss: 0.6058\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5589 - val_loss: 0.5902\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5457 - val_loss: 0.5765\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5338 - val_loss: 0.5644\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 0.5534\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5136 - val_loss: 0.5444\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5050 - val_loss: 0.5358\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4970 - val_loss: 0.5282\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4901 - val_loss: 0.5215\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4835 - val_loss: 0.5170\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4781 - val_loss: 0.5101\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4728 - val_loss: 0.5048\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4684 - val_loss: 0.5012\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4642 - val_loss: 0.4979\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4604 - val_loss: 0.4942\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4573 - val_loss: 0.4911\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4540 - val_loss: 0.4871\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4509 - val_loss: 0.4843\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4483 - val_loss: 0.4825\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4457 - val_loss: 0.4812\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4436 - val_loss: 0.4774\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4414 - val_loss: 0.4769\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4394 - val_loss: 0.4743\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4375 - val_loss: 0.4730\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4356 - val_loss: 0.4711\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4342 - val_loss: 0.4700\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4325 - val_loss: 0.4704\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4312 - val_loss: 0.4687\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4299 - val_loss: 0.4677\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.4659\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4276 - val_loss: 0.4646\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.4633\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.4624\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4247 - val_loss: 0.4622\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4239 - val_loss: 0.4616\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4230 - val_loss: 0.4603\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4220 - val_loss: 0.4607\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4214 - val_loss: 0.4585\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4205 - val_loss: 0.4576\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.4560\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4189 - val_loss: 0.4564\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4549\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4176 - val_loss: 0.4544\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4167 - val_loss: 0.4529\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4161 - val_loss: 0.4533\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.4536\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4147 - val_loss: 0.4530\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4141 - val_loss: 0.4520\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.4514\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.4504\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4124 - val_loss: 0.4491\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4115 - val_loss: 0.4473\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.4484\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4104 - val_loss: 0.4455\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4100 - val_loss: 0.4463\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4095 - val_loss: 0.4453\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4087 - val_loss: 0.4445\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4082 - val_loss: 0.4448\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4078 - val_loss: 0.4447\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4072 - val_loss: 0.4436\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4067 - val_loss: 0.4429\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4061 - val_loss: 0.4430\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4056 - val_loss: 0.4417\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4050 - val_loss: 0.4401\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4046 - val_loss: 0.4409\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.4404\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4035 - val_loss: 0.4389\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4032 - val_loss: 0.4381\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4028 - val_loss: 0.4382\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4022 - val_loss: 0.4367\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4019 - val_loss: 0.4372\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4014 - val_loss: 0.4369\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 0.4368\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4003 - val_loss: 0.4368\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3999 - val_loss: 0.4344\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3995 - val_loss: 0.4356\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3990 - val_loss: 0.4335\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3987 - val_loss: 0.4341\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.4328\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3979 - val_loss: 0.4324\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3971 - val_loss: 0.4335\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3971 - val_loss: 0.4328\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3964 - val_loss: 0.4320\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3964 - val_loss: 0.4319\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3956 - val_loss: 0.4304\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3953 - val_loss: 0.4311\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.4296\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.4298\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3942 - val_loss: 0.4299\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3939 - val_loss: 0.4288\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3933 - val_loss: 0.4280\n",
      "121/121 [==============================] - 0s 817us/step - loss: 0.4192\n",
      "[CV]  learning_rate=0.0007027974317461211, n_hidden=2, n_neurons=18, total=  34.1s\n",
      "[CV] learning_rate=0.0007027974317461211, n_hidden=2, n_neurons=18 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.2239 - val_loss: 1.9946\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5203 - val_loss: 1.2281\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0515 - val_loss: 0.9424\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8556 - val_loss: 0.8076\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7532 - val_loss: 0.7344\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6923 - val_loss: 0.6910\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6523 - val_loss: 0.6608\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6228 - val_loss: 0.6374\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5988 - val_loss: 0.6180\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5787 - val_loss: 0.6018\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5617 - val_loss: 0.5877\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5470 - val_loss: 0.5756\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5347 - val_loss: 0.5656\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 0.5563\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5141 - val_loss: 0.5488\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5060 - val_loss: 0.5409\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4987 - val_loss: 0.5346\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4921 - val_loss: 0.5289\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4864 - val_loss: 0.5259\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4813 - val_loss: 0.5190\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4772 - val_loss: 0.5148\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4727 - val_loss: 0.5106\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4689 - val_loss: 0.5072\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4653 - val_loss: 0.5037\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4624 - val_loss: 0.5007\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4592 - val_loss: 0.4983\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4568 - val_loss: 0.4959\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4543 - val_loss: 0.4936\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4518 - val_loss: 0.4916\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4497 - val_loss: 0.4893\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4477 - val_loss: 0.4880\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4457 - val_loss: 0.4855\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4438 - val_loss: 0.4849\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4423 - val_loss: 0.4824\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4406 - val_loss: 0.4809\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4800\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4374 - val_loss: 0.4788\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4360 - val_loss: 0.4771\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4346 - val_loss: 0.4752\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4333 - val_loss: 0.4746\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4318 - val_loss: 0.4733\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4307 - val_loss: 0.4723\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4295 - val_loss: 0.4717\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4285 - val_loss: 0.4713\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 0.4687\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4263 - val_loss: 0.4681\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4252 - val_loss: 0.4669\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4242 - val_loss: 0.4656\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4232 - val_loss: 0.4647\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4223 - val_loss: 0.4647\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4214 - val_loss: 0.4632\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4204 - val_loss: 0.4626\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4196 - val_loss: 0.4614\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4186 - val_loss: 0.4617\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4177 - val_loss: 0.4606\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4168 - val_loss: 0.4603\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4162 - val_loss: 0.4585\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4152 - val_loss: 0.4587\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4147 - val_loss: 0.4572\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4137 - val_loss: 0.4561\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4129 - val_loss: 0.4555\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4122 - val_loss: 0.4560\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4115 - val_loss: 0.4541\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.4542\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4101 - val_loss: 0.4530\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.4522\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4086 - val_loss: 0.4530\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4082 - val_loss: 0.4518\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4075 - val_loss: 0.4511\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.4502\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4061 - val_loss: 0.4500\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4056 - val_loss: 0.4493\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4049 - val_loss: 0.4479\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4045 - val_loss: 0.4478\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.4474\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4033 - val_loss: 0.4468\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4026 - val_loss: 0.4462\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4020 - val_loss: 0.4458\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.4456\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4009 - val_loss: 0.4446\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4005 - val_loss: 0.4441\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3998 - val_loss: 0.4436\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.4450\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3989 - val_loss: 0.4430\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3982 - val_loss: 0.4433\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3974 - val_loss: 0.4417\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3971 - val_loss: 0.4420\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3966 - val_loss: 0.4420\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3961 - val_loss: 0.4403\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3955 - val_loss: 0.4410\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3947 - val_loss: 0.4417\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3946 - val_loss: 0.4399\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3937 - val_loss: 0.4408\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.4383\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3930 - val_loss: 0.4381\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3923 - val_loss: 0.4370\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3919 - val_loss: 0.4371\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3914 - val_loss: 0.4370\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3910 - val_loss: 0.4361\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3903 - val_loss: 0.4358\n",
      "121/121 [==============================] - 0s 782us/step - loss: 0.4247\n",
      "[CV]  learning_rate=0.0007027974317461211, n_hidden=2, n_neurons=18, total=  34.1s\n",
      "[CV] learning_rate=0.0007027974317461211, n_hidden=2, n_neurons=18 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3344 - val_loss: 2.1259\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5648 - val_loss: 1.1332\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0418 - val_loss: 0.8797\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8617 - val_loss: 0.7849\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7778 - val_loss: 0.7378\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7303 - val_loss: 0.7069\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6982 - val_loss: 0.6839\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6739 - val_loss: 0.6637\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6538 - val_loss: 0.6474\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6361 - val_loss: 0.6329\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6219 - val_loss: 0.6191\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6086 - val_loss: 0.6076\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5969 - val_loss: 0.5975\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5868 - val_loss: 0.5883\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5776 - val_loss: 0.5797\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5693 - val_loss: 0.5726\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5621 - val_loss: 0.5647\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5550 - val_loss: 0.5587\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5491 - val_loss: 0.5528\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5436 - val_loss: 0.5488\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5390 - val_loss: 0.5439\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5343 - val_loss: 0.5398\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5302 - val_loss: 0.5358\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5261 - val_loss: 0.5331\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5229 - val_loss: 0.5292\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5196 - val_loss: 0.5261\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5165 - val_loss: 0.5233\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5137 - val_loss: 0.5206\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5111 - val_loss: 0.5180\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5086 - val_loss: 0.5155\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5061 - val_loss: 0.5130\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5040 - val_loss: 0.5112\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5017 - val_loss: 0.5087\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4999 - val_loss: 0.5069\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4979 - val_loss: 0.5053\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4960 - val_loss: 0.5039\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4943 - val_loss: 0.5023\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4923 - val_loss: 0.5016\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4912 - val_loss: 0.4996\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4896 - val_loss: 0.4975\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4881 - val_loss: 0.4967\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4866 - val_loss: 0.4958\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4858 - val_loss: 0.4938\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4842 - val_loss: 0.4928\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4830 - val_loss: 0.4916\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4818 - val_loss: 0.4904\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4808 - val_loss: 0.4895\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4795 - val_loss: 0.4885\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4784 - val_loss: 0.4877\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4776 - val_loss: 0.4866\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4764 - val_loss: 0.4859\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4754 - val_loss: 0.4850\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4741 - val_loss: 0.4842\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4737 - val_loss: 0.4830\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4725 - val_loss: 0.4823\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4715 - val_loss: 0.4814\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4706 - val_loss: 0.4814\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4699 - val_loss: 0.4800\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4691 - val_loss: 0.4791\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4681 - val_loss: 0.4782\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4672 - val_loss: 0.4778\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4664 - val_loss: 0.4772\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4656 - val_loss: 0.4764\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4648 - val_loss: 0.4755\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4640 - val_loss: 0.4744\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4632 - val_loss: 0.4743\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4624 - val_loss: 0.4732\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4617 - val_loss: 0.4726\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4610 - val_loss: 0.4719\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4602 - val_loss: 0.4708\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4594 - val_loss: 0.4704\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4587 - val_loss: 0.4699\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4580 - val_loss: 0.4694\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4573 - val_loss: 0.4685\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4566 - val_loss: 0.4679\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4558 - val_loss: 0.4673\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4553 - val_loss: 0.4666\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4544 - val_loss: 0.4665\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4536 - val_loss: 0.4654\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4530 - val_loss: 0.4649\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4524 - val_loss: 0.4640\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4517 - val_loss: 0.4636\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4508 - val_loss: 0.4633\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4504 - val_loss: 0.4626\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4497 - val_loss: 0.4619\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4489 - val_loss: 0.4618\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4484 - val_loss: 0.4606\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4476 - val_loss: 0.4602\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4468 - val_loss: 0.4601\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4463 - val_loss: 0.4590\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4455 - val_loss: 0.4583\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4448 - val_loss: 0.4581\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4441 - val_loss: 0.4573\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4435 - val_loss: 0.4569\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4429 - val_loss: 0.4564\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4420 - val_loss: 0.4564\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4416 - val_loss: 0.4555\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4410 - val_loss: 0.4545\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4404 - val_loss: 0.4538\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4395 - val_loss: 0.4547\n",
      "121/121 [==============================] - 0s 799us/step - loss: 0.4263\n",
      "[CV]  learning_rate=0.0007027974317461211, n_hidden=2, n_neurons=18, total=  34.0s\n",
      "[CV] learning_rate=0.010650914574384917, n_hidden=1, n_neurons=74 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8446 - val_loss: 0.6074\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5414 - val_loss: 0.5039\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4755 - val_loss: 0.4755\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4542 - val_loss: 0.4583\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4426 - val_loss: 0.4584\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4376 - val_loss: 0.4475\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4269 - val_loss: 0.4447\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4185 - val_loss: 0.4481\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4173 - val_loss: 0.4335\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4086 - val_loss: 0.4559\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4098 - val_loss: 0.4245\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4005 - val_loss: 0.4193\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4027 - val_loss: 0.4285\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3942 - val_loss: 0.4267\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3971 - val_loss: 0.4771\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3915 - val_loss: 0.4116\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3873 - val_loss: 0.4197\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3821 - val_loss: 0.4099\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3800 - val_loss: 0.4129\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3886 - val_loss: 0.4099\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3792 - val_loss: 0.4121\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3709 - val_loss: 0.4294\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.4017\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3682 - val_loss: 0.4082\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3689 - val_loss: 0.3958\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3668 - val_loss: 0.3985\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3672 - val_loss: 0.4446\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.4010\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.3897\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3578 - val_loss: 0.3958\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.3979\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.3914\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3594 - val_loss: 0.3895\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3664 - val_loss: 0.3875\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.4250\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.4237\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3867\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3851\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3824\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3438 - val_loss: 0.3869\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3443 - val_loss: 0.3817\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3569 - val_loss: 0.3766\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3408 - val_loss: 0.3782\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3405 - val_loss: 0.3752\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3388 - val_loss: 0.3768\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3413 - val_loss: 0.3790\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3384 - val_loss: 0.3716\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3372 - val_loss: 0.3736\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3341 - val_loss: 0.3772\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3369 - val_loss: 0.4095\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3358 - val_loss: 0.8274\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3344 - val_loss: 0.3702\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3392 - val_loss: 0.3760\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.3736\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3348 - val_loss: 0.3794\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3295 - val_loss: 0.4819\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3301 - val_loss: 0.3862\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3274 - val_loss: 0.3761\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3248 - val_loss: 0.3813\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3267 - val_loss: 0.3650\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3230 - val_loss: 0.3898\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3226 - val_loss: 0.3798\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.3728\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3222 - val_loss: 0.4894\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3333 - val_loss: 0.3681\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3207 - val_loss: 0.3668\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3407 - val_loss: 0.8207\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3295 - val_loss: 0.3704\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3227 - val_loss: 0.3883\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3190 - val_loss: 0.3642\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3165 - val_loss: 0.3609\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3176 - val_loss: 0.3964\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3295 - val_loss: 0.3697\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.3710\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3189 - val_loss: 0.3691\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3173 - val_loss: 0.3618\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3351 - val_loss: 0.3738\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3181 - val_loss: 0.3765\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3127 - val_loss: 0.3631\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3136 - val_loss: 0.3612\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3142 - val_loss: 0.3598\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3119 - val_loss: 0.3586\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3106 - val_loss: 0.3607\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3119 - val_loss: 0.3733\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3112 - val_loss: 0.3562\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3100 - val_loss: 0.3598\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3110 - val_loss: 0.4147\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3118 - val_loss: 0.3579\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3094 - val_loss: 0.3581\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3092 - val_loss: 0.3590\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3110 - val_loss: 0.3944\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3222 - val_loss: 0.3610\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3074 - val_loss: 0.4133\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3081 - val_loss: 0.3612\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3103 - val_loss: 0.3678\n",
      "121/121 [==============================] - 0s 792us/step - loss: 0.3287\n",
      "[CV]  learning_rate=0.010650914574384917, n_hidden=1, n_neurons=74, total=  31.6s\n",
      "[CV] learning_rate=0.010650914574384917, n_hidden=1, n_neurons=74 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7938 - val_loss: 0.6335\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.5113\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5128 - val_loss: 0.4970\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4608 - val_loss: 0.5039\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4998 - val_loss: 0.4784\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4486 - val_loss: 0.4548\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4220 - val_loss: 0.4469\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4121 - val_loss: 0.4410\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4072 - val_loss: 0.4346\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4005 - val_loss: 0.4432\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3974 - val_loss: 0.4245\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3910 - val_loss: 0.4205\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3887 - val_loss: 0.4219\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3835 - val_loss: 0.4174\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3813 - val_loss: 0.4536\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3814 - val_loss: 0.4122\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3762 - val_loss: 0.4123\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.4078\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3692 - val_loss: 0.4078\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.4109\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3681 - val_loss: 0.4006\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3627 - val_loss: 0.4017\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3623 - val_loss: 0.4069\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.4018\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3577 - val_loss: 0.3974\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3554 - val_loss: 0.4095\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3554 - val_loss: 0.3972\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3514 - val_loss: 0.3967\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3566 - val_loss: 0.3954\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3573 - val_loss: 0.3922\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3969\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3882\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3916\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3881\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3422 - val_loss: 0.3885\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3421 - val_loss: 0.4363\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3389 - val_loss: 0.4040\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3441 - val_loss: 0.3851\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3381 - val_loss: 0.3883\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3373 - val_loss: 0.3846\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3390 - val_loss: 0.3870\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3393 - val_loss: 0.3842\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3332 - val_loss: 0.3982\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3331 - val_loss: 0.3790\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.3819\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3384 - val_loss: 0.3796\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3318 - val_loss: 0.3796\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3286 - val_loss: 0.3803\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3276 - val_loss: 0.3755\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3271 - val_loss: 0.3811\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3321 - val_loss: 0.4631\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3273 - val_loss: 0.3758\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3354 - val_loss: 0.3750\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 0.3945\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3348 - val_loss: 0.3728\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3265 - val_loss: 0.8750\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.3744\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3205 - val_loss: 0.3706\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3208 - val_loss: 0.3740\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3304 - val_loss: 0.3754\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3209 - val_loss: 0.3706\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3187 - val_loss: 0.3762\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.3695\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3194 - val_loss: 0.4042\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3217 - val_loss: 0.3660\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3166 - val_loss: 0.3688\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3318 - val_loss: 0.3694\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3179 - val_loss: 0.3829\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3163 - val_loss: 0.3679\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3132 - val_loss: 0.3646\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3145 - val_loss: 0.3630\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3125 - val_loss: 0.3876\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3193 - val_loss: 0.3741\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3169 - val_loss: 0.3685\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3133 - val_loss: 0.3661\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3162 - val_loss: 0.3690\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3148 - val_loss: 0.3624\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3134 - val_loss: 0.3756\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3105 - val_loss: 0.3604\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3094 - val_loss: 0.3644\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3125 - val_loss: 0.3665\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3111 - val_loss: 0.3619\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3099 - val_loss: 0.3747\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3090 - val_loss: 0.3681\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3108 - val_loss: 0.3642\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3083 - val_loss: 0.3662\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3106 - val_loss: 0.3744\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3106 - val_loss: 0.3630\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3063 - val_loss: 0.3616\n",
      "121/121 [==============================] - 0s 830us/step - loss: 0.3349\n",
      "[CV]  learning_rate=0.010650914574384917, n_hidden=1, n_neurons=74, total=  32.5s\n",
      "[CV] learning_rate=0.010650914574384917, n_hidden=1, n_neurons=74 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9041 - val_loss: 2.6854\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 11.1238 - val_loss: 3.8965\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7869 - val_loss: 0.7033\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5170 - val_loss: 0.5739\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4676 - val_loss: 0.8461\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4497 - val_loss: 1.0708\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4353 - val_loss: 0.7675\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4268 - val_loss: 0.7897\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4184 - val_loss: 0.7455\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4122 - val_loss: 0.7762\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4078 - val_loss: 0.7834\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4040 - val_loss: 0.7871\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3988 - val_loss: 0.7930\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3937 - val_loss: 0.7281\n",
      "121/121 [==============================] - 0s 789us/step - loss: 0.4229\n",
      "[CV]  learning_rate=0.010650914574384917, n_hidden=1, n_neurons=74, total=   5.2s\n",
      "[CV] learning_rate=0.01848314861814289, n_hidden=1, n_neurons=95 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7397 - val_loss: 0.5756\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5182 - val_loss: 0.5029\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4591 - val_loss: 0.4575\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4409 - val_loss: 0.4505\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.4479\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4205 - val_loss: 0.4280\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4102 - val_loss: 0.4390\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4021 - val_loss: 0.4442\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4004 - val_loss: 0.4187\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3877 - val_loss: 0.4491\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.4108\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3786 - val_loss: 0.4000\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.4289\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3718 - val_loss: 0.4091\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3712 - val_loss: 0.4449\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3683 - val_loss: 0.3947\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3626 - val_loss: 0.3968\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3587 - val_loss: 0.4053\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.4148\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3776 - val_loss: 0.3902\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3573 - val_loss: 0.4115\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3513 - val_loss: 0.4334\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.4014\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.4013\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3851\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3439 - val_loss: 0.3842\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3449 - val_loss: 0.4091\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.3951\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3736 - val_loss: 0.3771\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3371 - val_loss: 0.3816\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3382 - val_loss: 0.3836\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3402 - val_loss: 0.3743\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 0.3714\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3807\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3326 - val_loss: 0.3900\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3298 - val_loss: 0.3836\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3312 - val_loss: 0.3776\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3283 - val_loss: 0.3733\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3256 - val_loss: 0.3630\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3228 - val_loss: 0.3715\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3248 - val_loss: 0.3621\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3238 - val_loss: 0.3613\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.3778\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3248 - val_loss: 0.3618\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3216 - val_loss: 0.3620\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3201 - val_loss: 0.3646\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3197 - val_loss: 0.3584\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3194 - val_loss: 0.3578\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3204 - val_loss: 0.3695\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3190 - val_loss: 0.3623\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3161 - val_loss: 0.4522\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3176 - val_loss: 0.3534\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3139 - val_loss: 0.3703\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3159 - val_loss: 0.3636\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3143 - val_loss: 0.3608\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3134 - val_loss: 0.3479\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3118 - val_loss: 0.3706\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3113 - val_loss: 0.3781\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3109 - val_loss: 0.3730\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3119 - val_loss: 0.3438\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3081 - val_loss: 0.3583\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3086 - val_loss: 0.3578\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3086 - val_loss: 0.3549\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3076 - val_loss: 0.4237\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3184 - val_loss: 0.3453\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3089 - val_loss: 0.3537\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3209 - val_loss: 0.4150\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3140 - val_loss: 0.3589\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3109 - val_loss: 0.3541\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3078 - val_loss: 0.3565\n",
      "121/121 [==============================] - 0s 783us/step - loss: 0.3496\n",
      "[CV]  learning_rate=0.01848314861814289, n_hidden=1, n_neurons=95, total=  23.7s\n",
      "[CV] learning_rate=0.01848314861814289, n_hidden=1, n_neurons=95 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3065 - val_loss: 0.7423\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5581 - val_loss: 0.4616\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.4472\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.4428\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3917 - val_loss: 0.4226\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.4140\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.4122\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.4100\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.4036\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3661 - val_loss: 0.4159\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3653 - val_loss: 0.3968\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3599 - val_loss: 0.3913\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.4056\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3935\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.5543\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.3924\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3871\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3432 - val_loss: 0.3832\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3403 - val_loss: 0.3891\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3439 - val_loss: 0.3869\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3412 - val_loss: 0.3763\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3344 - val_loss: 0.3802\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.3885\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3312 - val_loss: 0.3820\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3315 - val_loss: 0.3775\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3270 - val_loss: 0.3901\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3303 - val_loss: 0.3844\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3248 - val_loss: 0.3791\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3275 - val_loss: 0.3753\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3212 - val_loss: 0.3716\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3222 - val_loss: 0.3875\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3212 - val_loss: 0.3628\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3196 - val_loss: 0.3648\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3212 - val_loss: 0.3631\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3165 - val_loss: 0.3632\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3219 - val_loss: 0.5252\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3191 - val_loss: 0.3726\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3163 - val_loss: 0.3592\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3136 - val_loss: 0.3653\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3125 - val_loss: 0.3588\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3092 - val_loss: 0.3595\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3117 - val_loss: 0.3680\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3103 - val_loss: 0.3598\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3105 - val_loss: 0.3573\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3102 - val_loss: 0.3586\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3117 - val_loss: 0.3566\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3092 - val_loss: 0.3509\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3073 - val_loss: 0.3589\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3063 - val_loss: 0.3536\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3066 - val_loss: 0.3610\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3065 - val_loss: 0.3566\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3050 - val_loss: 0.3553\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3037 - val_loss: 0.3571\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3051 - val_loss: 0.3759\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3052 - val_loss: 0.3476\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3029 - val_loss: 0.3623\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3039 - val_loss: 0.3590\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3049 - val_loss: 0.4216\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3044 - val_loss: 0.3557\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3025 - val_loss: 0.3536\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3088 - val_loss: 0.3512\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3026 - val_loss: 0.3562\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3019 - val_loss: 0.3492\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2997 - val_loss: 0.3665\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3027 - val_loss: 0.3479\n",
      "121/121 [==============================] - 0s 782us/step - loss: 0.3288\n",
      "[CV]  learning_rate=0.01848314861814289, n_hidden=1, n_neurons=95, total=  21.9s\n",
      "[CV] learning_rate=0.01848314861814289, n_hidden=1, n_neurons=95 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7270 - val_loss: 0.8250\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0132 - val_loss: 50.0301\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 765us/step - loss: nan\n",
      "[CV]  learning_rate=0.01848314861814289, n_hidden=1, n_neurons=95, total=   4.1s\n",
      "[CV] learning_rate=0.02093484379537809, n_hidden=3, n_neurons=78 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7025 - val_loss: 0.4842\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4463 - val_loss: 0.4791\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4044 - val_loss: 0.4064\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3883 - val_loss: 0.4034\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3799 - val_loss: 0.4113\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3656 - val_loss: 0.3790\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3589 - val_loss: 0.3728\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3481 - val_loss: 0.4367\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3444 - val_loss: 0.3718\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3362 - val_loss: 0.3714\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3326 - val_loss: 0.3838\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3285 - val_loss: 0.3543\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3328 - val_loss: 0.4694\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3180 - val_loss: 0.3739\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3191 - val_loss: 0.3587\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3172 - val_loss: 0.3444\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3129 - val_loss: 0.3430\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3063 - val_loss: 0.3507\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3059 - val_loss: 0.3591\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3069 - val_loss: 0.3332\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3038 - val_loss: 0.3412\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2948 - val_loss: 0.3418\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2980 - val_loss: 0.3916\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2972 - val_loss: 0.3564\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2998 - val_loss: 0.3241\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2912 - val_loss: 0.3383\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2905 - val_loss: 0.3416\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2911 - val_loss: 0.3403\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2861 - val_loss: 0.3243\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2782 - val_loss: 0.3386\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2846 - val_loss: 0.3442\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2777 - val_loss: 0.3386\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2801 - val_loss: 0.3251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2746 - val_loss: 0.3415\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2718 - val_loss: 0.3530\n",
      "121/121 [==============================] - 0s 826us/step - loss: 0.3235\n",
      "[CV]  learning_rate=0.02093484379537809, n_hidden=3, n_neurons=78, total=  13.7s\n",
      "[CV] learning_rate=0.02093484379537809, n_hidden=3, n_neurons=78 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7091 - val_loss: 0.4889\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4295 - val_loss: 0.4514\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4052 - val_loss: 0.4388\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3900 - val_loss: 0.4230\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3754 - val_loss: 0.4036\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3692 - val_loss: 0.4139\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3623 - val_loss: 0.3947\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3462 - val_loss: 0.3953\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3456 - val_loss: 0.3675\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3352 - val_loss: 0.3740\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3322 - val_loss: 0.4064\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3295 - val_loss: 0.3538\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3188 - val_loss: 0.3986\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3166 - val_loss: 0.3679\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3156 - val_loss: 0.3586\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3110 - val_loss: 0.3553\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3088 - val_loss: 0.3468\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3032 - val_loss: 0.3413\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3010 - val_loss: 0.3471\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3008 - val_loss: 0.3423\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2959 - val_loss: 0.3471\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2957 - val_loss: 0.3592\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2935 - val_loss: 0.3736\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2885 - val_loss: 0.3373\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2874 - val_loss: 0.3463\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2798 - val_loss: 0.3331\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2833 - val_loss: 0.4069\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2801 - val_loss: 0.3303\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2810 - val_loss: 0.3346\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2757 - val_loss: 0.3362\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2770 - val_loss: 0.3291\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2720 - val_loss: 0.3179\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2740 - val_loss: 0.3329\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2741 - val_loss: 0.3199\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2693 - val_loss: 0.3257\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2747 - val_loss: 0.3286\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2628 - val_loss: 0.3351\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2651 - val_loss: 0.3196\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2646 - val_loss: 0.3176\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2661 - val_loss: 0.3132\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2599 - val_loss: 0.3145\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2579 - val_loss: 0.3345\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2615 - val_loss: 0.3114\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2630 - val_loss: 0.3187\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2583 - val_loss: 0.3075\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2614 - val_loss: 0.3334\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2570 - val_loss: 0.3235\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2558 - val_loss: 0.3221\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2543 - val_loss: 0.3121\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2533 - val_loss: 0.3157\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2523 - val_loss: 0.3069\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2494 - val_loss: 0.3194\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2479 - val_loss: 0.3096\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2528 - val_loss: 0.3188\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2483 - val_loss: 0.3304\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2477 - val_loss: 0.3187\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2494 - val_loss: 0.3162\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2505 - val_loss: 0.3136\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2490 - val_loss: 0.3240\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2444 - val_loss: 0.3096\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2464 - val_loss: 0.3116\n",
      "121/121 [==============================] - 0s 843us/step - loss: 0.2903\n",
      "[CV]  learning_rate=0.02093484379537809, n_hidden=3, n_neurons=78, total=  24.1s\n",
      "[CV] learning_rate=0.02093484379537809, n_hidden=3, n_neurons=78 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6804 - val_loss: 0.5292\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6002 - val_loss: 0.7339\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7016 - val_loss: 0.7329\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 856us/step - loss: nan\n",
      "[CV]  learning_rate=0.02093484379537809, n_hidden=3, n_neurons=78, total=   4.7s\n",
      "[CV] learning_rate=0.0007397534164346211, n_hidden=0, n_neurons=62 ...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 6.6393 - val_loss: 4.7885\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4292 - val_loss: 2.6448\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0385 - val_loss: 1.6901\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3863 - val_loss: 1.2289\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0614 - val_loss: 0.9935\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8913 - val_loss: 0.8670\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7976 - val_loss: 0.7939\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7426 - val_loss: 0.7484\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7078 - val_loss: 0.7176\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6841 - val_loss: 0.6952\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6667 - val_loss: 0.6780\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6529 - val_loss: 0.6638\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6416 - val_loss: 0.6520\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6320 - val_loss: 0.6415\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6234 - val_loss: 0.6324\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6158 - val_loss: 0.6243\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6089 - val_loss: 0.6171\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6026 - val_loss: 0.6106\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5968 - val_loss: 0.6047\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5916 - val_loss: 0.5993\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5867 - val_loss: 0.5942\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5821 - val_loss: 0.5897\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5779 - val_loss: 0.5857\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5740 - val_loss: 0.5818\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5704 - val_loss: 0.5783\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5670 - val_loss: 0.5751\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5639 - val_loss: 0.5720\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5610 - val_loss: 0.5693\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5582 - val_loss: 0.5667\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5557 - val_loss: 0.5644\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5533 - val_loss: 0.5623\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5511 - val_loss: 0.5602\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5490 - val_loss: 0.5583\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5471 - val_loss: 0.5565\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5453 - val_loss: 0.5550\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5436 - val_loss: 0.5536\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5420 - val_loss: 0.5521\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5405 - val_loss: 0.5509\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5391 - val_loss: 0.5497\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5378 - val_loss: 0.5486\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5365 - val_loss: 0.5475\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5354 - val_loss: 0.5465\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5344 - val_loss: 0.5455\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5334 - val_loss: 0.5448\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5324 - val_loss: 0.5441\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5316 - val_loss: 0.5434\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5308 - val_loss: 0.5427\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.5421\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5293 - val_loss: 0.5415\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5286 - val_loss: 0.5410\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5279 - val_loss: 0.5404\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5274 - val_loss: 0.5400\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5268 - val_loss: 0.5396\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5263 - val_loss: 0.5392\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5258 - val_loss: 0.5390\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5253 - val_loss: 0.5386\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5249 - val_loss: 0.5383\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 0.5381\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5241 - val_loss: 0.5379\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 0.5376\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 0.5373\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 0.5371\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5228 - val_loss: 0.5368\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5226 - val_loss: 0.5367\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5223 - val_loss: 0.5366\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.5365\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5219 - val_loss: 0.5363\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5216 - val_loss: 0.5362\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5214 - val_loss: 0.5361\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5213 - val_loss: 0.5360\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5211 - val_loss: 0.5360\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5209 - val_loss: 0.5359\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5207 - val_loss: 0.5358\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5206 - val_loss: 0.5358\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5205 - val_loss: 0.5358\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5204 - val_loss: 0.5357\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5203 - val_loss: 0.5357\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5201 - val_loss: 0.5356\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5200 - val_loss: 0.5355\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5200 - val_loss: 0.5356\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5198 - val_loss: 0.5356\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5197 - val_loss: 0.5355\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5197 - val_loss: 0.5355\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5196 - val_loss: 0.5356\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5195 - val_loss: 0.5355\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5194 - val_loss: 0.5354\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5194 - val_loss: 0.5354\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5194 - val_loss: 0.5354\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5193 - val_loss: 0.5354\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5192 - val_loss: 0.5355\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5192 - val_loss: 0.5355\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5192 - val_loss: 0.5355\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5191 - val_loss: 0.5356\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5191 - val_loss: 0.5356\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5190 - val_loss: 0.5354\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5190 - val_loss: 0.5354\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5189 - val_loss: 0.5355\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5190 - val_loss: 0.5355\n",
      "121/121 [==============================] - 0s 799us/step - loss: 0.5134\n",
      "[CV]  learning_rate=0.0007397534164346211, n_hidden=0, n_neurons=62, total=  31.5s\n",
      "[CV] learning_rate=0.0007397534164346211, n_hidden=0, n_neurons=62 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3286 - val_loss: 3.1579\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3649 - val_loss: 1.8612\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4670 - val_loss: 1.2577\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0412 - val_loss: 0.9687\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8338 - val_loss: 0.8282\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7308 - val_loss: 0.7597\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6783 - val_loss: 0.7239\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6502 - val_loss: 0.7045\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6340 - val_loss: 0.6935\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6241 - val_loss: 0.6843\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6169 - val_loss: 0.6791\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6115 - val_loss: 0.6732\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6068 - val_loss: 0.6680\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6026 - val_loss: 0.6640\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5985 - val_loss: 0.6579\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5951 - val_loss: 0.6561\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5917 - val_loss: 0.6525\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5884 - val_loss: 0.6487\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5851 - val_loss: 0.6435\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5822 - val_loss: 0.6423\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5794 - val_loss: 0.6381\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5766 - val_loss: 0.6347\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5738 - val_loss: 0.6327\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5713 - val_loss: 0.6287\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5689 - val_loss: 0.6256\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5664 - val_loss: 0.6230\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5642 - val_loss: 0.6201\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5620 - val_loss: 0.6187\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5598 - val_loss: 0.6169\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5578 - val_loss: 0.6134\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5560 - val_loss: 0.6107\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5540 - val_loss: 0.6093\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5521 - val_loss: 0.6050\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5505 - val_loss: 0.6036\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5489 - val_loss: 0.6023\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5473 - val_loss: 0.6001\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5457 - val_loss: 0.5974\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5442 - val_loss: 0.5952\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5428 - val_loss: 0.5937\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5414 - val_loss: 0.5923\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5400 - val_loss: 0.5917\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5388 - val_loss: 0.5903\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5376 - val_loss: 0.5882\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5363 - val_loss: 0.5854\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5352 - val_loss: 0.5845\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5341 - val_loss: 0.5824\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5331 - val_loss: 0.5816\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5320 - val_loss: 0.5814\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5793\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.5776\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5292 - val_loss: 0.5773\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5283 - val_loss: 0.5758\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5275 - val_loss: 0.5747\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5267 - val_loss: 0.5731\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5258 - val_loss: 0.5730\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 0.5713\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 0.5711\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 0.5695\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 0.5685\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5223 - val_loss: 0.5682\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5217 - val_loss: 0.5674\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5211 - val_loss: 0.5666\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5205 - val_loss: 0.5660\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5198 - val_loss: 0.5637\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5194 - val_loss: 0.5632\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5189 - val_loss: 0.5628\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5183 - val_loss: 0.5616\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5179 - val_loss: 0.5609\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5174 - val_loss: 0.5607\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5170 - val_loss: 0.5593\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5166 - val_loss: 0.5588\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5161 - val_loss: 0.5588\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5157 - val_loss: 0.5587\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5153 - val_loss: 0.5577\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5151 - val_loss: 0.5569\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5146 - val_loss: 0.5566\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5141 - val_loss: 0.5554\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5139 - val_loss: 0.5553\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5135 - val_loss: 0.5551\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5132 - val_loss: 0.5551\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5130 - val_loss: 0.5543\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5126 - val_loss: 0.5536\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5124 - val_loss: 0.5529\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5121 - val_loss: 0.5523\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5118 - val_loss: 0.5521\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5115 - val_loss: 0.5522\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5113 - val_loss: 0.5521\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5111 - val_loss: 0.5516\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5109 - val_loss: 0.5508\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5106 - val_loss: 0.5509\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5104 - val_loss: 0.5501\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5103 - val_loss: 0.5497\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5100 - val_loss: 0.5486\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5098 - val_loss: 0.5486\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5096 - val_loss: 0.5489\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5095 - val_loss: 0.5490\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5092 - val_loss: 0.5483\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5091 - val_loss: 0.5484\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5090 - val_loss: 0.5479\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5088 - val_loss: 0.5475\n",
      "121/121 [==============================] - 0s 790us/step - loss: 0.5466\n",
      "[CV]  learning_rate=0.0007397534164346211, n_hidden=0, n_neurons=62, total=  32.2s\n",
      "[CV] learning_rate=0.0007397534164346211, n_hidden=0, n_neurons=62 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3066 - val_loss: 3.2259\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5759 - val_loss: 2.0358\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7280 - val_loss: 1.4524\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3033 - val_loss: 1.1616\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0842 - val_loss: 1.0100\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9661 - val_loss: 0.9278\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8983 - val_loss: 0.8795\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8554 - val_loss: 0.8468\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8256 - val_loss: 0.8236\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8029 - val_loss: 0.8048\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7840 - val_loss: 0.7892\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7676 - val_loss: 0.7742\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7530 - val_loss: 0.7609\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7394 - val_loss: 0.7488\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7269 - val_loss: 0.7375\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7152 - val_loss: 0.7265\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7044 - val_loss: 0.7159\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6941 - val_loss: 0.7057\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6845 - val_loss: 0.6966\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6755 - val_loss: 0.6883\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6671 - val_loss: 0.6806\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6591 - val_loss: 0.6732\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6517 - val_loss: 0.6657\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6446 - val_loss: 0.6594\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6380 - val_loss: 0.6530\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6318 - val_loss: 0.6469\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6260 - val_loss: 0.6413\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6205 - val_loss: 0.6363\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6153 - val_loss: 0.6313\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6104 - val_loss: 0.6265\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6058 - val_loss: 0.6219\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6015 - val_loss: 0.6178\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5974 - val_loss: 0.6135\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5936 - val_loss: 0.6099\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5900 - val_loss: 0.6065\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5866 - val_loss: 0.6033\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5833 - val_loss: 0.6003\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5803 - val_loss: 0.5974\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5774 - val_loss: 0.5945\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5747 - val_loss: 0.5917\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5722 - val_loss: 0.5896\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5697 - val_loss: 0.5873\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5675 - val_loss: 0.5847\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5653 - val_loss: 0.5826\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5633 - val_loss: 0.5810\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5613 - val_loss: 0.5786\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5596 - val_loss: 0.5771\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5578 - val_loss: 0.5755\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5562 - val_loss: 0.5739\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5547 - val_loss: 0.5724\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5532 - val_loss: 0.5710\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5518 - val_loss: 0.5696\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5505 - val_loss: 0.5684\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5493 - val_loss: 0.5669\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5481 - val_loss: 0.5659\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5470 - val_loss: 0.5645\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5459 - val_loss: 0.5637\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5450 - val_loss: 0.5626\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5440 - val_loss: 0.5618\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5431 - val_loss: 0.5609\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5423 - val_loss: 0.5601\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5414 - val_loss: 0.5598\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5407 - val_loss: 0.5588\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5399 - val_loss: 0.5575\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5392 - val_loss: 0.5565\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5386 - val_loss: 0.5560\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5379 - val_loss: 0.5553\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5373 - val_loss: 0.5547\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5367 - val_loss: 0.5540\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5362 - val_loss: 0.5532\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5357 - val_loss: 0.5528\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5352 - val_loss: 0.5525\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5347 - val_loss: 0.5522\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5343 - val_loss: 0.5519\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5339 - val_loss: 0.5512\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5334 - val_loss: 0.5510\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5331 - val_loss: 0.5501\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5327 - val_loss: 0.5499\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 0.5497\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5320 - val_loss: 0.5495\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5317 - val_loss: 0.5489\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5314 - val_loss: 0.5485\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.5480\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5308 - val_loss: 0.5477\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 0.5474\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5303 - val_loss: 0.5471\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 0.5468\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5298 - val_loss: 0.5464\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5295 - val_loss: 0.5463\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5294 - val_loss: 0.5461\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5292 - val_loss: 0.5458\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5289 - val_loss: 0.5456\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 0.5450\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5286 - val_loss: 0.5450\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5284 - val_loss: 0.5451\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5282 - val_loss: 0.5452\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5281 - val_loss: 0.5451\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5279 - val_loss: 0.5447\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5278 - val_loss: 0.5447\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5277 - val_loss: 0.5443\n",
      "121/121 [==============================] - 0s 776us/step - loss: 0.5026\n",
      "[CV]  learning_rate=0.0007397534164346211, n_hidden=0, n_neurons=62, total=  34.9s\n",
      "[CV] learning_rate=0.001342090852386379, n_hidden=3, n_neurons=82 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.8109 - val_loss: 0.9419\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7938 - val_loss: 0.7585\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7039 - val_loss: 0.6977\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6588 - val_loss: 0.6560\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6267\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5988 - val_loss: 0.6005\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5765 - val_loss: 0.5804\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5573 - val_loss: 0.5622\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5410 - val_loss: 0.5485\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 0.5384\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5149 - val_loss: 0.5263\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5043 - val_loss: 0.5164\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4950 - val_loss: 0.5098\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4864 - val_loss: 0.5010\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4790 - val_loss: 0.4955\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4721 - val_loss: 0.4888\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4658 - val_loss: 0.4836\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4601 - val_loss: 0.4791\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4547 - val_loss: 0.4786\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4503 - val_loss: 0.4699\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4456 - val_loss: 0.4664\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4411 - val_loss: 0.4630\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4377 - val_loss: 0.4599\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4336 - val_loss: 0.4595\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4311 - val_loss: 0.4539\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4272 - val_loss: 0.4506\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4241 - val_loss: 0.4467\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4210 - val_loss: 0.4439\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4178 - val_loss: 0.4409\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4152 - val_loss: 0.4387\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4124 - val_loss: 0.4376\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4099 - val_loss: 0.4340\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4077 - val_loss: 0.4321\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4049 - val_loss: 0.4312\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4027 - val_loss: 0.4280\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4005 - val_loss: 0.4266\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3985 - val_loss: 0.4237\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3963 - val_loss: 0.4232\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3946 - val_loss: 0.4211\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3923 - val_loss: 0.4190\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3902 - val_loss: 0.4168\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3888 - val_loss: 0.4148\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3870 - val_loss: 0.4136\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3856 - val_loss: 0.4124\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3837 - val_loss: 0.4110\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3817 - val_loss: 0.4114\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3806 - val_loss: 0.4102\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3790 - val_loss: 0.4101\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3777 - val_loss: 0.4052\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3760 - val_loss: 0.4051\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3744 - val_loss: 0.4040\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3733 - val_loss: 0.4025\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.4025\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3706 - val_loss: 0.4010\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3690 - val_loss: 0.4004\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3678 - val_loss: 0.4012\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3664 - val_loss: 0.4026\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3657 - val_loss: 0.3977\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3643 - val_loss: 0.3957\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3633 - val_loss: 0.3944\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3615 - val_loss: 0.3935\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3608 - val_loss: 0.3935\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3595 - val_loss: 0.3912\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3583 - val_loss: 0.3922\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3578 - val_loss: 0.3906\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3560 - val_loss: 0.3911\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3550 - val_loss: 0.3895\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3541 - val_loss: 0.3878\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3530 - val_loss: 0.3873\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3513 - val_loss: 0.3876\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3507 - val_loss: 0.3866\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3497 - val_loss: 0.3873\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3488 - val_loss: 0.3853\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3476 - val_loss: 0.3839\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3464 - val_loss: 0.3867\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3458 - val_loss: 0.3835\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3446 - val_loss: 0.3816\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3437 - val_loss: 0.3816\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3428 - val_loss: 0.3794\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3422 - val_loss: 0.3819\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3414 - val_loss: 0.3794\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3399 - val_loss: 0.3790\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3385 - val_loss: 0.3802\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3382 - val_loss: 0.3783\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3374 - val_loss: 0.3762\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3364 - val_loss: 0.3750\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3353 - val_loss: 0.3761\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3353 - val_loss: 0.3743\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3341 - val_loss: 0.3729\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3323 - val_loss: 0.3755\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3320 - val_loss: 0.3738\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3311 - val_loss: 0.3740\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3310 - val_loss: 0.3736\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3301 - val_loss: 0.3719\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3290 - val_loss: 0.3717\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3282 - val_loss: 0.3702\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3270 - val_loss: 0.3700\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3276 - val_loss: 0.3704\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3259 - val_loss: 0.3693\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3251 - val_loss: 0.3685\n",
      "121/121 [==============================] - 0s 844us/step - loss: 0.3419\n",
      "[CV]  learning_rate=0.001342090852386379, n_hidden=3, n_neurons=82, total=  39.3s\n",
      "[CV] learning_rate=0.001342090852386379, n_hidden=3, n_neurons=82 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.3776 - val_loss: 1.0252\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8009 - val_loss: 0.6991\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6464 - val_loss: 0.6430\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5978 - val_loss: 0.6085\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5640 - val_loss: 0.5796\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5367 - val_loss: 0.5557\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5145 - val_loss: 0.5351\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4960 - val_loss: 0.5186\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4810 - val_loss: 0.5045\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4687 - val_loss: 0.4930\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4581 - val_loss: 0.4830\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4497 - val_loss: 0.4743\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4421 - val_loss: 0.4676\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4357 - val_loss: 0.4612\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4302 - val_loss: 0.4552\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4249 - val_loss: 0.4497\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4206 - val_loss: 0.4457\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4160 - val_loss: 0.4404\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4123 - val_loss: 0.4374\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4086 - val_loss: 0.4342\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4056 - val_loss: 0.4297\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4028 - val_loss: 0.4261\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3993 - val_loss: 0.4241\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3970 - val_loss: 0.4218\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3947 - val_loss: 0.4193\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3913 - val_loss: 0.4200\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3898 - val_loss: 0.4151\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3875 - val_loss: 0.4146\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3852 - val_loss: 0.4115\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3833 - val_loss: 0.4109\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3812 - val_loss: 0.4080\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3798 - val_loss: 0.4062\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3774 - val_loss: 0.4056\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3761 - val_loss: 0.4038\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3741 - val_loss: 0.4021\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3725 - val_loss: 0.4008\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3701 - val_loss: 0.4011\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3695 - val_loss: 0.3980\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3676 - val_loss: 0.3974\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3659 - val_loss: 0.3955\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3639 - val_loss: 0.3954\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3627 - val_loss: 0.3945\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3623 - val_loss: 0.3930\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3608 - val_loss: 0.3924\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3591 - val_loss: 0.3913\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3577 - val_loss: 0.3902\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3568 - val_loss: 0.3910\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3549 - val_loss: 0.3887\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3536 - val_loss: 0.3874\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3523 - val_loss: 0.3876\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3515 - val_loss: 0.3882\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3502 - val_loss: 0.3865\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3494 - val_loss: 0.3857\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3511 - val_loss: 0.3852\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3464 - val_loss: 0.3839\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3446 - val_loss: 0.3857\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3442 - val_loss: 0.3831\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3430 - val_loss: 0.3819\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3418 - val_loss: 0.3811\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3404 - val_loss: 0.3795\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3394 - val_loss: 0.3801\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3388 - val_loss: 0.3803\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3407 - val_loss: 0.3770\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3359 - val_loss: 0.3803\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3376 - val_loss: 0.3764\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3339 - val_loss: 0.3752\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3335 - val_loss: 0.3751\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3336 - val_loss: 0.3765\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3332 - val_loss: 0.3766\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3334 - val_loss: 0.3745\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3308 - val_loss: 0.3731\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3288 - val_loss: 0.3735\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3279 - val_loss: 0.3718\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3267 - val_loss: 0.3707\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3263 - val_loss: 0.3719\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3257 - val_loss: 0.3699\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3244 - val_loss: 0.3709\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3228 - val_loss: 0.3700\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3226 - val_loss: 0.3684\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3253 - val_loss: 0.3690\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3208 - val_loss: 0.3679\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3212 - val_loss: 0.3681\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3187 - val_loss: 0.3674\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3181 - val_loss: 0.3672\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3186 - val_loss: 0.3671\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3185 - val_loss: 0.3675\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3216 - val_loss: 0.3669\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3150 - val_loss: 0.3659\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3137 - val_loss: 0.3635\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3137 - val_loss: 0.3654\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3122 - val_loss: 0.3647\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3122 - val_loss: 0.3678\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3111 - val_loss: 0.3647\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3116 - val_loss: 0.3635\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3106 - val_loss: 0.3612\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3092 - val_loss: 0.3617\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3091 - val_loss: 0.3611\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3114 - val_loss: 0.3622\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3071 - val_loss: 0.3602\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3059 - val_loss: 0.3620\n",
      "121/121 [==============================] - 0s 894us/step - loss: 0.3394\n",
      "[CV]  learning_rate=0.001342090852386379, n_hidden=3, n_neurons=82, total=  39.5s\n",
      "[CV] learning_rate=0.001342090852386379, n_hidden=3, n_neurons=82 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.0042 - val_loss: 0.8753\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7113 - val_loss: 0.6523\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6250 - val_loss: 0.6055\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 0.5740\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5519 - val_loss: 0.5478\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5277 - val_loss: 0.5282\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5073 - val_loss: 0.5141\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4915 - val_loss: 0.4998\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4782 - val_loss: 0.4877\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4665 - val_loss: 0.4792\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4582 - val_loss: 0.4713\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4506 - val_loss: 0.4654\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4436 - val_loss: 0.4606\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4383 - val_loss: 0.4552\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4331 - val_loss: 0.4529\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4289 - val_loss: 0.4451\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4250 - val_loss: 0.4426\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4205 - val_loss: 0.4396\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4179 - val_loss: 0.4352\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4145 - val_loss: 0.4347\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4119 - val_loss: 0.4316\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4090 - val_loss: 0.4293\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4064 - val_loss: 0.4263\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4036 - val_loss: 0.4277\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4012 - val_loss: 0.4230\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3993 - val_loss: 0.4211\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3973 - val_loss: 0.4203\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3952 - val_loss: 0.4165\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3928 - val_loss: 0.4159\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3909 - val_loss: 0.4150\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3894 - val_loss: 0.4117\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3874 - val_loss: 0.4131\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3851 - val_loss: 0.4115\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3840 - val_loss: 0.4093\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3823 - val_loss: 0.4079\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3804 - val_loss: 0.4056\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3786 - val_loss: 0.4077\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3770 - val_loss: 0.4036\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3754 - val_loss: 0.4035\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3745 - val_loss: 0.4006\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3722 - val_loss: 0.4033\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3716 - val_loss: 0.4000\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3705 - val_loss: 0.3981\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3682 - val_loss: 0.4001\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3677 - val_loss: 0.3945\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3662 - val_loss: 0.3953\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3651 - val_loss: 0.3946\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3631 - val_loss: 0.3922\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3621 - val_loss: 0.3919\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3613 - val_loss: 0.3906\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3594 - val_loss: 0.3899\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3589 - val_loss: 0.3895\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3570 - val_loss: 0.3871\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3561 - val_loss: 0.3869\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3549 - val_loss: 0.3865\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3539 - val_loss: 0.3851\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3519 - val_loss: 0.3878\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3517 - val_loss: 0.3831\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3503 - val_loss: 0.3825\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3491 - val_loss: 0.3808\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3479 - val_loss: 0.3801\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3463 - val_loss: 0.3814\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3457 - val_loss: 0.3822\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3449 - val_loss: 0.3789\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3433 - val_loss: 0.3777\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3426 - val_loss: 0.3763\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3411 - val_loss: 0.3748\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3405 - val_loss: 0.3768\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3394 - val_loss: 0.3758\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3383 - val_loss: 0.3729\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3371 - val_loss: 0.3719\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3364 - val_loss: 0.3710\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3352 - val_loss: 0.3713\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3339 - val_loss: 0.3690\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3330 - val_loss: 0.3696\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3322 - val_loss: 0.3678\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3313 - val_loss: 0.3677\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3299 - val_loss: 0.3697\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3292 - val_loss: 0.3665\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3283 - val_loss: 0.3674\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3274 - val_loss: 0.3665\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3268 - val_loss: 0.3653\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3253 - val_loss: 0.3661\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3256 - val_loss: 0.3650\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3251 - val_loss: 0.3636\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3238 - val_loss: 0.3660\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3243 - val_loss: 0.3626\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3248 - val_loss: 0.3658\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3258 - val_loss: 0.3668\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3272 - val_loss: 0.3645\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3250 - val_loss: 0.3633\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3239 - val_loss: 0.3648\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3223 - val_loss: 0.3608\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3214 - val_loss: 0.3586\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3184 - val_loss: 0.3575\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3172 - val_loss: 0.3602\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3157 - val_loss: 0.3579\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3145 - val_loss: 0.3559\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3132 - val_loss: 0.3550\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3122 - val_loss: 0.3580\n",
      "121/121 [==============================] - 0s 824us/step - loss: 0.3296\n",
      "[CV]  learning_rate=0.001342090852386379, n_hidden=3, n_neurons=82, total=  38.6s\n",
      "[CV] learning_rate=0.003601683277857631, n_hidden=3, n_neurons=26 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3388 - val_loss: 0.7419\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6727 - val_loss: 0.6436\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6082 - val_loss: 0.5986\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5653 - val_loss: 0.5628\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5315 - val_loss: 0.5354\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5032 - val_loss: 0.5114\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4811 - val_loss: 0.4933\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4638 - val_loss: 0.4814\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4528 - val_loss: 0.4708\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4436 - val_loss: 0.4708\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4352 - val_loss: 0.4569\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4285 - val_loss: 0.4513\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.4511\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4169 - val_loss: 0.4406\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4122 - val_loss: 0.4399\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.4348\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.4325\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3997 - val_loss: 0.4285\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3956 - val_loss: 0.4302\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.4209\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3885 - val_loss: 0.4198\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3840 - val_loss: 0.4213\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3825 - val_loss: 0.4164\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.4188\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3781 - val_loss: 0.4109\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.4097\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3724 - val_loss: 0.4068\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3694 - val_loss: 0.4062\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3676 - val_loss: 0.4004\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3648 - val_loss: 0.4034\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3630 - val_loss: 0.4015\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3612 - val_loss: 0.4002\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3596 - val_loss: 0.3967\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.3963\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3961\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3533 - val_loss: 0.3968\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3929\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3921\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3905\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3951\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3445 - val_loss: 0.3857\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3428 - val_loss: 0.3860\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3411 - val_loss: 0.3842\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3402 - val_loss: 0.3815\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3393 - val_loss: 0.3811\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3374 - val_loss: 0.3860\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3365 - val_loss: 0.3790\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.3805\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.3771\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3323 - val_loss: 0.3796\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3315 - val_loss: 0.3759\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3307 - val_loss: 0.3770\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3281 - val_loss: 0.3776\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3284 - val_loss: 0.3720\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3267 - val_loss: 0.3784\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3261 - val_loss: 0.3767\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3249 - val_loss: 0.3856\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3241 - val_loss: 0.3731\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3230 - val_loss: 0.3690\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3227 - val_loss: 0.3674\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3202 - val_loss: 0.3729\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.3688\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3184 - val_loss: 0.3669\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3179 - val_loss: 0.3763\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3191 - val_loss: 0.3643\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3162 - val_loss: 0.3688\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3155 - val_loss: 0.3645\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3150 - val_loss: 0.3656\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3136 - val_loss: 0.3618\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3116 - val_loss: 0.3646\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3107 - val_loss: 0.3623\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3099 - val_loss: 0.3677\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3091 - val_loss: 0.3644\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3080 - val_loss: 0.3565\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3068 - val_loss: 0.3608\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3063 - val_loss: 0.3647\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3054 - val_loss: 0.3566\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3043 - val_loss: 0.3626\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3031 - val_loss: 0.3631\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3043 - val_loss: 0.3548\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3025 - val_loss: 0.3531\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3008 - val_loss: 0.3529\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3002 - val_loss: 0.3564\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2991 - val_loss: 0.3606\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2986 - val_loss: 0.3491\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2977 - val_loss: 0.3498\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2968 - val_loss: 0.3493\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2980 - val_loss: 0.3484\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2965 - val_loss: 0.3472\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2944 - val_loss: 0.3488\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2940 - val_loss: 0.3519\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2947 - val_loss: 0.3472\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2940 - val_loss: 0.3543\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2945 - val_loss: 0.3479\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2922 - val_loss: 0.3477\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2919 - val_loss: 0.3431\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2918 - val_loss: 0.3403\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2927 - val_loss: 0.3432\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2901 - val_loss: 0.3396\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2897 - val_loss: 0.3565\n",
      "121/121 [==============================] - 0s 787us/step - loss: 0.3279\n",
      "[CV]  learning_rate=0.003601683277857631, n_hidden=3, n_neurons=26, total=  35.0s\n",
      "[CV] learning_rate=0.003601683277857631, n_hidden=3, n_neurons=26 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.9288 - val_loss: 0.8254\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6398 - val_loss: 0.5864\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5501 - val_loss: 0.5505\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5063 - val_loss: 0.5124\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4732 - val_loss: 0.4820\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4480 - val_loss: 0.4620\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4298 - val_loss: 0.4487\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4152 - val_loss: 0.4364\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4044 - val_loss: 0.4286\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3965 - val_loss: 0.4192\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3897 - val_loss: 0.4145\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3848 - val_loss: 0.4107\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.4100\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3752 - val_loss: 0.4069\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3723 - val_loss: 0.4011\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3680 - val_loss: 0.3980\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3653 - val_loss: 0.4000\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3607 - val_loss: 0.3920\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.3914\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.3950\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3534 - val_loss: 0.3870\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3506 - val_loss: 0.3850\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3881\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3832\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3439 - val_loss: 0.3809\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3390 - val_loss: 0.3903\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3397 - val_loss: 0.3794\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3374 - val_loss: 0.3775\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3357 - val_loss: 0.3774\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3337 - val_loss: 0.3786\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3325 - val_loss: 0.3733\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3319 - val_loss: 0.3711\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3297 - val_loss: 0.3729\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3288 - val_loss: 0.3706\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3265 - val_loss: 0.3688\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3266 - val_loss: 0.3671\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.3717\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3227 - val_loss: 0.3662\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3223 - val_loss: 0.3675\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3205 - val_loss: 0.3634\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3181 - val_loss: 0.3655\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3183 - val_loss: 0.3663\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3170 - val_loss: 0.3610\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3172 - val_loss: 0.3623\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.3649\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3151 - val_loss: 0.3587\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3146 - val_loss: 0.3593\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3127 - val_loss: 0.3647\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3113 - val_loss: 0.3565\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3115 - val_loss: 0.3602\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3113 - val_loss: 0.3621\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3095 - val_loss: 0.3592\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3101 - val_loss: 0.3580\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3135 - val_loss: 0.3603\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3076 - val_loss: 0.3558\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3057 - val_loss: 0.3537\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3056 - val_loss: 0.3568\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3058 - val_loss: 0.3534\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3053 - val_loss: 0.3586\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3037 - val_loss: 0.3495\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3036 - val_loss: 0.3549\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3025 - val_loss: 0.3587\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3027 - val_loss: 0.3522\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3012 - val_loss: 0.3527\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3023 - val_loss: 0.3511\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3009 - val_loss: 0.3538\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3008 - val_loss: 0.3472\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3015 - val_loss: 0.3475\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2998 - val_loss: 0.3500\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2997 - val_loss: 0.3468\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2984 - val_loss: 0.3456\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2978 - val_loss: 0.3534\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2975 - val_loss: 0.3469\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2970 - val_loss: 0.3510\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2978 - val_loss: 0.3473\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2978 - val_loss: 0.3513\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2951 - val_loss: 0.3529\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2959 - val_loss: 0.3485\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2957 - val_loss: 0.3429\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2962 - val_loss: 0.3496\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2947 - val_loss: 0.3440\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2933 - val_loss: 0.3456\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2914 - val_loss: 0.3479\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2918 - val_loss: 0.3454\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2933 - val_loss: 0.3428\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2912 - val_loss: 0.3513\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2948 - val_loss: 0.3479\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2907 - val_loss: 0.3445\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2906 - val_loss: 0.3490\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2897 - val_loss: 0.3407\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2892 - val_loss: 0.3414\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2903 - val_loss: 0.3533\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2880 - val_loss: 0.3427\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2892 - val_loss: 0.3408\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2887 - val_loss: 0.3419\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2881 - val_loss: 0.3441\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2873 - val_loss: 0.3393\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2876 - val_loss: 0.3424\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2863 - val_loss: 0.3397\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2871 - val_loss: 0.3539\n",
      "121/121 [==============================] - 0s 813us/step - loss: 0.3322\n",
      "[CV]  learning_rate=0.003601683277857631, n_hidden=3, n_neurons=26, total=  35.1s\n",
      "[CV] learning_rate=0.003601683277857631, n_hidden=3, n_neurons=26 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.5880 - val_loss: 0.7272\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6813 - val_loss: 0.6171\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5805 - val_loss: 0.5554\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5324 - val_loss: 0.5207\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5018 - val_loss: 0.4938\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4797 - val_loss: 0.4762\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4619 - val_loss: 0.4668\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4492 - val_loss: 0.4517\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4381 - val_loss: 0.4436\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4284 - val_loss: 0.4383\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4218 - val_loss: 0.4310\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.4283\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4089 - val_loss: 0.4204\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4043 - val_loss: 0.4225\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3998 - val_loss: 0.4211\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3953 - val_loss: 0.4118\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3919 - val_loss: 0.4120\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3872 - val_loss: 0.4066\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.3991\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3813 - val_loss: 0.4055\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.4019\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3755 - val_loss: 0.3970\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3724 - val_loss: 0.3923\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3700 - val_loss: 0.3990\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3665 - val_loss: 0.3910\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3648 - val_loss: 0.3857\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.3902\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3610 - val_loss: 0.3825\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3579 - val_loss: 0.3842\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.3827\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.3768\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3531 - val_loss: 0.3806\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3761\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3760\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3757\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3453 - val_loss: 0.3694\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3429 - val_loss: 0.3861\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3416 - val_loss: 0.3683\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3395 - val_loss: 0.3698\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3394 - val_loss: 0.3664\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3359 - val_loss: 0.3680\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3369 - val_loss: 0.3772\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3361 - val_loss: 0.3801\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3333 - val_loss: 0.3684\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.3604\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3327 - val_loss: 0.3646\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3303 - val_loss: 0.3694\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.3636\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3284 - val_loss: 0.3594\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3282 - val_loss: 0.3577\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.3593\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3255 - val_loss: 0.3656\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 0.3604\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3235 - val_loss: 0.3591\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3223 - val_loss: 0.3593\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3213 - val_loss: 0.3550\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3188 - val_loss: 0.3617\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.3513\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3185 - val_loss: 0.3530\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3179 - val_loss: 0.3585\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3170 - val_loss: 0.3533\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3142 - val_loss: 0.3558\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3154 - val_loss: 0.3552\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3145 - val_loss: 0.3519\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3123 - val_loss: 0.3620\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3136 - val_loss: 0.3537\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3118 - val_loss: 0.3522\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3118 - val_loss: 0.3540\n",
      "121/121 [==============================] - 0s 792us/step - loss: 0.3252\n",
      "[CV]  learning_rate=0.003601683277857631, n_hidden=3, n_neurons=26, total=  24.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 14.3min finished\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f9ce9d8ef10>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-b3a0e12aa696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m rnd_search_cv.fit(X_train, y_train, epochs=100,\n\u001b[0m\u001b[1;32m     12\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                   callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;31m# we clone again after setting params in case some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;31m# of the params are estimators as well.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[1;32m    762\u001b[0m                 **self.best_params_))\n\u001b[1;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[1;32m     97\u001b[0m                                \u001b[0;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                                (estimator, name))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f9ce9d8ef10>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnd_search_cv.best_estimator_.model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"final_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"final_model.h5\")\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
